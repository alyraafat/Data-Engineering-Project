{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window, Row, Column, functions as fn\n",
    "import psutil\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load the dataset from the parquet format given in the google drive above\n",
    "- Load the dataset.\n",
    "- Preview first 20 rows.\n",
    "- How many partitions is this dataframe split into?\n",
    "- Change partitions to be equal to the number of your logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"M3\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "#.config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.3.jar\").master(\"local\")\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", 'true')\n",
    "# spark.config(\"spark.memory.offHeap.enabled\",\"true\") \n",
    "# spark.config(\"spark.memory.offHeap.size\",\"10g\")\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# spark context to interact with the driver\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './work/fintech_data_29_52_1008.parquet'\n",
    "fintech_df = spark.read.parquet(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         Customer Id|           Emp Title|Emp Length|Home Ownership|Annual Inc|Annual Inc Joint|Verification Status|Zip Code|Addr State|Avg Cur Bal|Tot Cur Bal|Loan Id|Loan Status|Loan Amount|State|Funded Amount|      Term|Int Rate|Grade|       Issue Date|Pymnt Plan|      Type|           Purpose|         Description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidceDkzalx4YmRce...|     president/owner| 10+ years|      MORTGAGE|   80000.0|            NULL|    Source Verified|   333xx|        FL|     8275.0|   239986.0|  39474|    Current|     6000.0|   FL|       6000.0| 36 months|  0.0649|    4|   14 August 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yic0SVx4ZjRceGZlX...|PETTY OFFICER FIR...| 10+ years|      MORTGAGE|   75384.0|            NULL|    Source Verified|   237xx|        VA|    25385.0|   279232.0| 158200|    Current|    15000.0|   VA|      15000.0| 60 months|  0.1806|   20|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGUyXHgxZVx4M...|               nyco |   4 years|           OWN|   33800.0|            NULL|           Verified|   111xx|        NY|      282.0|     1691.0| 113752| Fully Paid|    11500.0|   NY|      11500.0| 36 months|  0.1114|    7| 12 December 2012|     false|INDIVIDUAL|             other|         Family Help|\n",
      "|YidcXD1ceDg2XHhhZ...|Nippon Express US...| 10+ years|           OWN|   50000.0|            NULL|           Verified|   070xx|        NJ|    14458.0|   130124.0| 181412| Fully Paid|    18000.0|   NJ|      18000.0| 60 months|  0.2149|   23| 12 November 2012|     false|INDIVIDUAL|       credit_card|  Credit Card Payoff|\n",
      "|YidceDgyXHgwNiZce...|  Operations Manager|   2 years|      MORTGAGE|   75000.0|        160000.0|       Not Verified|   750xx|        TX|    27703.0|   443252.0| 227090|    Current|    25000.0|   TX|      25000.0| 60 months|  0.1171|   10|19 September 2019|     false| Joint App|  home_improvement|    Home improvement|\n",
      "|YidceGU4XHhmYVdIX...|    flight attendant| 10+ years|      MORTGAGE|  110000.0|            NULL|           Verified|   762xx|        TX|     4799.0|   196782.0| 220157| Fully Paid|    24000.0|   TX|      24000.0| 60 months|  0.1899|   19|  17 January 2017|     false|Individual|  home_improvement|    Home improvement|\n",
      "|YicmXHg4Mlx4ODY/X...|       SALES MANAGER| 10+ years|          RENT|  100000.0|            NULL|    Source Verified|   452xx|        OH|     3627.0|    29019.0| 149100| Fully Paid|    15000.0|   OH|      15000.0| 36 months|  0.0967|    8| 13 November 2013|     false|INDIVIDUAL|debt_consolidation|  A BIRD IN THE HAND|\n",
      "|YiJceDgwXHg5ZSdGN...|maintenance mechanic|   4 years|           OWN|   70000.0|            NULL|           Verified|   125xx|        NY|     2599.0|    18193.0| 152910|    Current|    15000.0|   NY|      15000.0| 36 months|  0.1499|   14| 16 December 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YidceDBjXHhkZlx4Z...|       Administrator| 10+ years|      MORTGAGE|  117400.0|            NULL|    Source Verified|   082xx|        NJ|     9975.0|   259343.0| 214955| Fully Paid|    24000.0|   NJ|      24000.0| 36 months|  0.0692|    5|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidqXHhiMlx4ZDZaX...|   Senior Consultant|   2 years|          RENT|   85000.0|            NULL|       Not Verified|   105xx|        NY|     4182.0|    66907.0|   7032| Fully Paid|     2500.0|   NY|       2500.0| 36 months|  0.0799|    4|    17 March 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGE2XHg5NVx4Z...|             Manager|   6 years|          RENT|  110000.0|            NULL|       Not Verified|   342xx|        FL|     6631.0|    72939.0| 184546|    Current|    19175.0|   FL|      19175.0| 36 months|   0.288|   19|    19 April 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yic9K1x4MDhceGU2X...|Senior Structural...|    1 year|      MORTGAGE|   70000.0|            NULL|       Not Verified|   786xx|        TX|    24167.0|   265836.0| 167217|    Current|    16000.0|   TX|      16000.0| 60 months|  0.1288|   14| 15 December 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDk5XHhlMFx4M...|      Police Officer| 10+ years|          RENT|   70000.0|            NULL|       Not Verified|   337xx|        FL|     4993.0|    44934.0|   4621| Fully Paid|     2000.0|   FL|       2000.0| 36 months|  0.1299|   11|  15 January 2015|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidceDA1XHg4MGomX...|     Project Manager| 10+ years|          RENT|   55000.0|            NULL|    Source Verified|   372xx|        TN|     8173.0|    81725.0|  19826|    Current|     4000.0|   TN|       4000.0| 36 months|  0.1747|   18|     18 June 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidDXHg4Mlx4YWZce...|    Sales associate |   2 years|      MORTGAGE|   23000.0|            NULL|    Source Verified|   357xx|        AL|     7833.0|    23500.0|  28527|    Current|     5000.0|   AL|       5000.0| 36 months|  0.1042|    6|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidibFx4OTRceGI4X...|            Director|   5 years|      MORTGAGE|  120000.0|            NULL|       Not Verified|   925xx|        CA|    30724.0|   368683.0|  51952| Fully Paid|     7000.0|   CA|       7000.0| 36 months|  0.0624|    1| 15 November 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJXUjpceDg2XHg5O...|             Manager| 10+ years|      MORTGAGE|   82000.0|            NULL|       Not Verified|   357xx|        AL|    28152.0|   281524.0|  50695|    Current|     6600.0|   AL|       6600.0| 36 months|  0.1335|   12| 14 November 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGZmMj58XHhkY...|Network Administr...|   4 years|      MORTGAGE|   79000.0|            NULL|    Source Verified|   989xx|        WA|    12693.0|   165015.0| 256148| Fully Paid|    35000.0|   WA|      35000.0| 36 months|  0.1367|   12| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceDg0QVx4YThce...|       Staff Account|    1 year|      MORTGAGE|   35000.0|            NULL|       Not Verified|   331xx|        FL|     9902.0|   108921.0|  41140|    Current|     6000.0|   FL|       6000.0| 36 months|  0.0867|   10| 14 November 2014|     false|INDIVIDUAL|  home_improvement|    Home improvement|\n",
      "|Yic4Q1hceGI2XHg4Y...|           Inspector|   7 years|          RENT|   70000.0|            NULL|       Not Verified|   770xx|        TX|     3255.0|    32550.0| 152067| Fully Paid|    15000.0|   TX|      15000.0| 36 months|  0.1333|   15| 15 February 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(fintech_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical cores: 24\n",
      "Physical cores: 12\n"
     ]
    }
   ],
   "source": [
    "logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "physical_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "\n",
    "print(f\"Logical cores: {logical_cores}\")\n",
    "print(f\"Physical cores: {physical_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitioned_fintech_df = fintech_df.repartition(logical_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitioned_fintech_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rename all columns (replacing a space with an underscore, and making it lowercase)\n",
    "- Detect missing    \n",
    "  -  Create a function that takes in the df and returns any data structrue of your choice(df/dict,list,tuple,etc) which has the name of the column and percentage of missing entries from the whole dataset.\n",
    "  - Tip : storing the missing info as dict where the key is the column name and value is the percentage would be the easiest.\n",
    "  - Prinout the missing info\n",
    "- Handle missing\n",
    "  - For numerical features replace with 0.\n",
    "  - For categorical/strings replace with mode\n",
    "- Check missing\n",
    "  - Afterwards, check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer Id',\n",
       " 'Emp Title',\n",
       " 'Emp Length',\n",
       " 'Home Ownership',\n",
       " 'Annual Inc',\n",
       " 'Annual Inc Joint',\n",
       " 'Verification Status',\n",
       " 'Zip Code',\n",
       " 'Addr State',\n",
       " 'Avg Cur Bal',\n",
       " 'Tot Cur Bal',\n",
       " 'Loan Id',\n",
       " 'Loan Status',\n",
       " 'Loan Amount',\n",
       " 'State',\n",
       " 'Funded Amount',\n",
       " 'Term',\n",
       " 'Int Rate',\n",
       " 'Grade',\n",
       " 'Issue Date',\n",
       " 'Pymnt Plan',\n",
       " 'Type',\n",
       " 'Purpose',\n",
       " 'Description']"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitioned_fintech_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_fintech_df = repartitioned_fintech_df.toDF(*[col.replace(\" \", \"_\").lower() for col in repartitioned_fintech_df.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'annual_inc_joint',\n",
       " 'verification_status',\n",
       " 'zip_code',\n",
       " 'addr_state',\n",
       " 'avg_cur_bal',\n",
       " 'tot_cur_bal',\n",
       " 'loan_id',\n",
       " 'loan_status',\n",
       " 'loan_amount',\n",
       " 'state',\n",
       " 'funded_amount',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'grade',\n",
       " 'issue_date',\n",
       " 'pymnt_plan',\n",
       " 'type',\n",
       " 'purpose',\n",
       " 'description']"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_fintech_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         customer_id|           emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|    loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|       issue_date|pymnt_plan|      type|           purpose|         description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YicgXHgxOVx4ODU+X...| bartender/ waitress|   3 years|          RENT|   35000.0|            NULL|    Source Verified|   191xx|        PA|     2368.0|    16578.0| 139672|     Fully Paid|    14000.0|   PA|      14000.0| 36 months|  0.1999|   19|    17 March 2017|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidJXHhiOFx4OWRce...|      Line- Operator|  < 1 year|           OWN|   65000.0|            NULL|    Source Verified|   458xx|        OH|     1263.0|     3789.0| 207602|In Grace Period|    21275.0|   OH|      21275.0| 60 months|   0.124|    6|     19 July 2019|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidceGQwXHhmNl1ce...|                NULL|  < 1 year|      MORTGAGE|  100000.0|            NULL|           Verified|   152xx|        PA|    26234.0|   341037.0| 230358|     Fully Paid|    25350.0|   PA|      25350.0| 36 months|  0.1727|   19|    16 April 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yid6XHhkMlx4YmR8e...|   Corporate Trainer|   5 years|      MORTGAGE|   41000.0|            NULL|    Source Verified|   322xx|        FL|     3152.0|    18912.0|  98856|        Current|    10000.0|   FL|      10000.0| 36 months|     0.2|   16| 18 February 2018|     false|Individual|  home_improvement|    Home improvement|\n",
      "|Yid8XHhkZlpvdFJce...|Certified Applicator|   7 years|          RENT|   30000.0|            NULL|           Verified|   788xx|        TX|     2451.0|    19607.0|  33846|     Fully Paid|     5000.0|   TX|       5000.0| 36 months|  0.1899|   17| 17 February 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDk5XHhmY1x4M...|Associate Researcher|   4 years|           OWN|   67000.0|            NULL|       Not Verified|   598xx|        MT|     6283.0|    31415.0|  21295|     Fully Paid|     4250.0|   MT|       4250.0| 36 months|  0.1561|   20|    15 April 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDBiQlx4OWVce...|               sales| 10+ years|          RENT|   60000.0|            NULL|    Source Verified|   330xx|        FL|     1487.0|    19336.0| 167841|    Charged Off|    16000.0|   FL|      16000.0| 60 months|  0.1431|   14|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yid1Nlx4OTRceGQ1X...|Utility Plant  En...| 10+ years|      MORTGAGE|   82000.0|            NULL|       Not Verified|   088xx|        NJ|    30457.0|   335023.0| 235327|     Fully Paid|    28000.0|   NJ|      28000.0| 36 months|  0.0789|    2|   15 August 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yic5XHgxM1x4ZGRce...|      Office Manager|  < 1 year|      MORTGAGE|  125000.0|            NULL|    Source Verified|   346xx|        FL|     8382.0|   125734.0| 193947|        Current|    20000.0|   FL|      20000.0| 36 months|  0.1349|   15|16 September 2016|     false|INDIVIDUAL|  home_improvement|    Home improvement|\n",
      "|YidceGNiPFZ3XHhkN...|       Sales Manager|   2 years|          RENT|   40000.0|            NULL|       Not Verified|   606xx|        IL|     2656.0|    10624.0|  95200|     Fully Paid|    10000.0|   IL|      10000.0| 36 months|  0.1398|   12| 13 December 2013|     false|INDIVIDUAL|debt_consolidation|Good Bet Debt Con...|\n",
      "|YidceGZkOFx4ZTZce...|Logistics coordin...|   3 years|          RENT|   47800.0|            NULL|           Verified|   902xx|        CA|     2855.0|    14276.0|  40657|        Current|     6000.0|   CA|       6000.0| 36 months|  0.0797|    3|      17 May 2017|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDE3TzQuXHhhY...|           Echo tech|  < 1 year|          RENT|   64999.0|            NULL|       Not Verified|   481xx|        MI|     4442.0|   133252.0|  20325|        Current|     4050.0|   MI|       4050.0| 36 months|  0.1102|   10| 19 November 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidLXHhjMFx4ZDVce...|      LOAN PROCESSOR| 10+ years|      MORTGAGE|   75000.0|            NULL|           Verified|   365xx|        AL|    12223.0|   293351.0| 158526|        Current|    15000.0|   AL|      15000.0| 60 months|  0.1953|   20|    16 April 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YicoXHg5MFx4ZjFce...|       HR consultant|   8 years|      MORTGAGE|   75000.0|            NULL|       Not Verified|   840xx|        UT|    16220.0|   243302.0| 186598|        Current|    19825.0|   UT|      19825.0| 60 months|  0.2399|   29|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidDXHhjMDVceDFlX...|Director of Stude...|    1 year|          RENT|   71937.0|            NULL|    Source Verified|   852xx|        AZ|     9603.0|   105631.0| 231429|        Current|    26000.0|   AZ|      26000.0| 36 months|  0.1499|   13|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGJjXHhiNVx4Y...|    Building Manager| 10+ years|          RENT|   51000.0|            NULL|       Not Verified|   801xx|        CO|     3237.0|    16184.0|   1664|        Current|     1200.0|   CO|       1200.0| 36 months|  0.1942|   16|     18 June 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFkXHg4ZVx4O...|Human Resource Ma...|   9 years|      MORTGAGE|   58000.0|            NULL|       Not Verified|   146xx|        NY|     7212.0|   122608.0|  70546|        Current|     8400.0|   NY|       8400.0| 36 months|  0.1049|    7| 16 November 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yiczflx4YjUqXHhlN...|             Teacher|  < 1 year|      MORTGAGE|  120000.0|            NULL|           Verified|   891xx|        NV|    20113.0|   201125.0| 253983|     Fully Paid|    35000.0|   NV|      35000.0| 36 months|  0.0724|    1|  17 January 2017|     false|Individual|  home_improvement|    Home improvement|\n",
      "|YiJyJ1x4ODZceDAwX...| Deportation officer| 10+ years|      MORTGAGE|  115000.0|            NULL|           Verified|   919xx|        CA|    66538.0|   598839.0| 243836|     Fully Paid|    30000.0|   CA|      30000.0| 60 months|  0.1008|    6|18 September 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicrXHgxOFx4Y2Nce...|Internet Sales Co...|  < 1 year|      MORTGAGE|   37000.0|            NULL|       Not Verified|   805xx|        CO|    22918.0|   206261.0|  86629|     Fully Paid|    10000.0|   CO|      10000.0| 36 months|  0.0818|    7|  15 October 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_fintech_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing(df: pyspark.sql.dataframe.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detect missing values in a PySpark DataFrame and calculate the percentage of missing entries.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are column names and values are percentages of missing values.\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    missing_info = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(fn.col(column).isNull()).count()\n",
    "        missing_percentage = (missing_count / total_rows) * 100\n",
    "        missing_info[column] = missing_percentage\n",
    "    \n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dict = detect_missing(renamed_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 8.612652608213097,\n",
       " 'emp_length': 6.777654458009619,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 93.11875693673696,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 4.384017758046615,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.8139104698483166}"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- loan_id: long (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- funded_amount: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- grade: long (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- pymnt_plan: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_fintech_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annual_inc',\n",
       " 'annual_inc_joint',\n",
       " 'avg_cur_bal',\n",
       " 'tot_cur_bal',\n",
       " 'loan_id',\n",
       " 'loan_amount',\n",
       " 'funded_amount',\n",
       " 'int_rate',\n",
       " 'grade']"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_types = (pyspark.sql.types.DoubleType, pyspark.sql.types.FloatType, pyspark.sql.types.IntegerType, pyspark.sql.types.LongType)\n",
    "numerical_columns = [field.name for field in renamed_fintech_df.schema.fields if isinstance(field.dataType, numerical_types)]\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def handle_missing_numerical(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace missing values in numerical columns with 0.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with missing values in numerical columns replaced by 0.\n",
    "    \"\"\"\n",
    "    missing_dict = detect_missing(df)\n",
    "    numerical_types = (pyspark.sql.types.DoubleType, pyspark.sql.types.FloatType, pyspark.sql.types.IntegerType, pyspark.sql.types.LongType)\n",
    "    numerical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, numerical_types)]\n",
    "    for column in numerical_columns:\n",
    "        if missing_dict[column] > 0:\n",
    "            df = df.fillna(value=0, subset=[column])\n",
    "            print(df.select(column).distinct().show())\n",
    "            print('-'*50)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|annual_inc_joint|\n",
      "+----------------+\n",
      "|        300000.0|\n",
      "|        147000.0|\n",
      "|         90000.0|\n",
      "|    218124.59375|\n",
      "|        100000.0|\n",
      "|             0.0|\n",
      "|        175000.0|\n",
      "|        110000.0|\n",
      "|        130000.0|\n",
      "|         98677.0|\n",
      "|        115000.0|\n",
      "|        116100.0|\n",
      "|         53000.0|\n",
      "|         95000.0|\n",
      "|        118000.0|\n",
      "|         75000.0|\n",
      "|        123000.0|\n",
      "|        142000.0|\n",
      "|         81475.0|\n",
      "|        121500.0|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "--------------------------------------------------\n",
      "+--------+\n",
      "|int_rate|\n",
      "+--------+\n",
      "|  0.1147|\n",
      "|  0.1892|\n",
      "|  0.1699|\n",
      "|  0.2437|\n",
      "|  0.1356|\n",
      "|  0.1992|\n",
      "|  0.2582|\n",
      "|     0.0|\n",
      "|  0.1167|\n",
      "|   0.234|\n",
      "|  0.1612|\n",
      "|  0.1825|\n",
      "|   0.109|\n",
      "|  0.0624|\n",
      "|  0.1524|\n",
      "|  0.1446|\n",
      "|  0.0739|\n",
      "|  0.1942|\n",
      "|  0.2565|\n",
      "|  0.2199|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "numerically_imputed_df = handle_missing_numerical(renamed_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 8.612652608213097,\n",
       " 'emp_length': 6.777654458009619,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 0.0,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 0.0,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.8139104698483166}"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict_updated = detect_missing(numerically_imputed_df)\n",
    "missing_dict_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_categorical(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace missing values in categorical/string columns with the mode (most frequent value).\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with missing values in categorical columns replaced by mode.\n",
    "    \"\"\"\n",
    "    missing_dict = detect_missing(df)\n",
    "    categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, pyspark.sql.types.StringType)]\n",
    "    for column in categorical_columns:\n",
    "        if missing_dict[column] > 0:\n",
    "            mode_col = df.filter(fn.col(column).isNotNull()).groupBy(column).count().orderBy(fn.col('count').desc()).limit(1)\n",
    "            mode_col.show()\n",
    "            mode_value = mode_col.select(column).collect()[0][0]\n",
    "            print(f\"Mode value for {column}: {mode_value}\")\n",
    "            df = df.fillna(value= mode_value, subset=[column])\n",
    "            print('-'* 50)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|emp_title|count|\n",
      "+---------+-----+\n",
      "|  Teacher|  468|\n",
      "+---------+-----+\n",
      "\n",
      "Mode value for emp_title: Teacher\n",
      "--------------------------------------------------\n",
      "+----------+-----+\n",
      "|emp_length|count|\n",
      "+----------+-----+\n",
      "| 10+ years| 8855|\n",
      "+----------+-----+\n",
      "\n",
      "Mode value for emp_length: 10+ years\n",
      "--------------------------------------------------\n",
      "+------------------+-----+\n",
      "|       description|count|\n",
      "+------------------+-----+\n",
      "|Debt consolidation|14421|\n",
      "+------------------+-----+\n",
      "\n",
      "Mode value for description: Debt consolidation\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "categorically_imputed_df = handle_missing_categorical(numerically_imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 0.0,\n",
       " 'emp_length': 0.0,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 0.0,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 0.0,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.0}"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict_updated = detect_missing(categorically_imputed_df)\n",
    "missing_dict_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no missing values are remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode only the following categorical values\n",
    "- Emp Length: Change to numerical\n",
    "- Home Ownership: One Hot Encoding\n",
    "- Verification Status: One Hot Encoding\n",
    "- State: Label Encoding\n",
    "- Type: One Hot Encoding\n",
    "- Purpose: Label Encoding\n",
    "- For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "DO NOT Encode the employment title of description or any other column that is not\n",
    "mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LOOKUP_TABLE = spark.createDataFrame([], schema=\"original_column STRING, original_value STRING, encoded_column STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OWN'"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorically_imputed_df.select('home_ownership').distinct().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|home_ownership|\n",
      "+--------------+\n",
      "|           OWN|\n",
      "|          RENT|\n",
      "|      MORTGAGE|\n",
      "|           ANY|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorically_imputed_df.select(\"home_ownership\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pyspark.sql.dataframe.DataFrame, columns: List[str]) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding to the specified categorical columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    columns (list): List of column names to one-hot encode.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    lookup_data = []\n",
    "    for column in columns:\n",
    "        unique_values = df.select(fn.lower(column)).distinct().collect()\n",
    "        # lowercase every value in unique_values\n",
    "        # print(unique_values[0][0])\n",
    "        # unique_values = [str(value[column]) for value in unique_values]\n",
    "        # show counts before ohe\n",
    "        df.groupBy(fn.lower(column)).count().show()\n",
    "\n",
    "        for row in unique_values:\n",
    "            curr_value = row[0]\n",
    "            encoded_col_name = f\"{column}_{curr_value}\"\n",
    "            print(f\"Encoding {column} - {curr_value} as {encoded_col_name}\")\n",
    "            df = df.withColumn(encoded_col_name, fn.when(fn.lower(fn.col(column)) == curr_value, 1).otherwise(0))\n",
    "            df = df.withColumn(encoded_col_name, df[encoded_col_name].cast(\"int\"))\n",
    "            lookup_data.append(Row(original_column=column, original_value=curr_value, encoded_column=encoded_col_name))\n",
    "            # show counts for each encoded column\n",
    "            df.groupBy(encoded_col_name).count().show()\n",
    "            print('-'*50)\n",
    "    \n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    print(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "    df = df.drop(*columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|lower(home_ownership)|count|\n",
      "+---------------------+-----+\n",
      "|                  own| 3060|\n",
      "|                  any|   35|\n",
      "|             mortgage|13364|\n",
      "|                 rent|10571|\n",
      "+---------------------+-----+\n",
      "\n",
      "Encoding home_ownership - own as home_ownership_own\n",
      "+------------------+-----+\n",
      "|home_ownership_own|count|\n",
      "+------------------+-----+\n",
      "|                 1| 3060|\n",
      "|                 0|23970|\n",
      "+------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding home_ownership - any as home_ownership_any\n",
      "+------------------+-----+\n",
      "|home_ownership_any|count|\n",
      "+------------------+-----+\n",
      "|                 1|   35|\n",
      "|                 0|26995|\n",
      "+------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding home_ownership - mortgage as home_ownership_mortgage\n",
      "+-----------------------+-----+\n",
      "|home_ownership_mortgage|count|\n",
      "+-----------------------+-----+\n",
      "|                      1|13364|\n",
      "|                      0|13666|\n",
      "+-----------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding home_ownership - rent as home_ownership_rent\n",
      "+-------------------+-----+\n",
      "|home_ownership_rent|count|\n",
      "+-------------------+-----+\n",
      "|                  1|10571|\n",
      "|                  0|16459|\n",
      "+-------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "+--------------------------+-----+\n",
      "|lower(verification_status)|count|\n",
      "+--------------------------+-----+\n",
      "|              not verified| 9737|\n",
      "|                  verified| 6650|\n",
      "|           source verified|10643|\n",
      "+--------------------------+-----+\n",
      "\n",
      "Encoding verification_status - not verified as verification_status_not verified\n",
      "+--------------------------------+-----+\n",
      "|verification_status_not verified|count|\n",
      "+--------------------------------+-----+\n",
      "|                               1| 9737|\n",
      "|                               0|17293|\n",
      "+--------------------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding verification_status - verified as verification_status_verified\n",
      "+----------------------------+-----+\n",
      "|verification_status_verified|count|\n",
      "+----------------------------+-----+\n",
      "|                           1| 6650|\n",
      "|                           0|20380|\n",
      "+----------------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding verification_status - source verified as verification_status_source verified\n",
      "+-----------------------------------+-----+\n",
      "|verification_status_source verified|count|\n",
      "+-----------------------------------+-----+\n",
      "|                                  1|10643|\n",
      "|                                  0|16387|\n",
      "+-----------------------------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "+-----------+-----+\n",
      "|lower(type)|count|\n",
      "+-----------+-----+\n",
      "|      joint|  101|\n",
      "| direct_pay|   39|\n",
      "|  joint app| 1759|\n",
      "| individual|25131|\n",
      "+-----------+-----+\n",
      "\n",
      "Encoding type - joint as type_joint\n",
      "+----------+-----+\n",
      "|type_joint|count|\n",
      "+----------+-----+\n",
      "|         1|  101|\n",
      "|         0|26929|\n",
      "+----------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding type - direct_pay as type_direct_pay\n",
      "+---------------+-----+\n",
      "|type_direct_pay|count|\n",
      "+---------------+-----+\n",
      "|              1|   39|\n",
      "|              0|26991|\n",
      "+---------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding type - joint app as type_joint app\n",
      "+--------------+-----+\n",
      "|type_joint app|count|\n",
      "+--------------+-----+\n",
      "|             1| 1759|\n",
      "|             0|25271|\n",
      "+--------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Encoding type - individual as type_individual\n",
      "+---------------+-----+\n",
      "|type_individual|count|\n",
      "+---------------+-----+\n",
      "|              1|25131|\n",
      "|              0| 1899|\n",
      "+---------------+-----+\n",
      "\n",
      "--------------------------------------------------\n",
      "[Row(original_column='home_ownership', original_value='own', encoded_column='home_ownership_own'), Row(original_column='home_ownership', original_value='any', encoded_column='home_ownership_any'), Row(original_column='home_ownership', original_value='mortgage', encoded_column='home_ownership_mortgage'), Row(original_column='home_ownership', original_value='rent', encoded_column='home_ownership_rent'), Row(original_column='verification_status', original_value='not verified', encoded_column='verification_status_not verified'), Row(original_column='verification_status', original_value='verified', encoded_column='verification_status_verified'), Row(original_column='verification_status', original_value='source verified', encoded_column='verification_status_source verified'), Row(original_column='type', original_value='joint', encoded_column='type_joint'), Row(original_column='type', original_value='direct_pay', encoded_column='type_direct_pay'), Row(original_column='type', original_value='joint app', encoded_column='type_joint app'), Row(original_column='type', original_value='individual', encoded_column='type_individual')]\n"
     ]
    }
   ],
   "source": [
    "ohe_columns = [\"home_ownership\", \"verification_status\", \"type\"]\n",
    "ohe_fintech_df = one_hot_encode(categorically_imputed_df, ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['customer_id',\n",
       "  'emp_title',\n",
       "  'emp_length',\n",
       "  'annual_inc',\n",
       "  'annual_inc_joint',\n",
       "  'zip_code',\n",
       "  'addr_state',\n",
       "  'avg_cur_bal',\n",
       "  'tot_cur_bal',\n",
       "  'loan_id',\n",
       "  'loan_status',\n",
       "  'loan_amount',\n",
       "  'state',\n",
       "  'funded_amount',\n",
       "  'term',\n",
       "  'int_rate',\n",
       "  'grade',\n",
       "  'issue_date',\n",
       "  'pymnt_plan',\n",
       "  'purpose',\n",
       "  'description',\n",
       "  'home_ownership_own',\n",
       "  'home_ownership_any',\n",
       "  'home_ownership_mortgage',\n",
       "  'home_ownership_rent',\n",
       "  'verification_status_not verified',\n",
       "  'verification_status_verified',\n",
       "  'verification_status_source verified',\n",
       "  'type_joint',\n",
       "  'type_direct_pay',\n",
       "  'type_joint app',\n",
       "  'type_individual'],\n",
       " 32)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_fintech_df.columns, len(ohe_fintech_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|home_ownership_mortgage|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      0|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe_fintech_df.select(['home_ownership_mortgage']).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state='AK'),\n",
       " Row(state='AL'),\n",
       " Row(state='AR'),\n",
       " Row(state='AZ'),\n",
       " Row(state='CA'),\n",
       " Row(state='CO'),\n",
       " Row(state='CT'),\n",
       " Row(state='DC'),\n",
       " Row(state='DE'),\n",
       " Row(state='FL'),\n",
       " Row(state='GA'),\n",
       " Row(state='HI'),\n",
       " Row(state='ID'),\n",
       " Row(state='IL'),\n",
       " Row(state='IN'),\n",
       " Row(state='KS'),\n",
       " Row(state='KY'),\n",
       " Row(state='LA'),\n",
       " Row(state='MA'),\n",
       " Row(state='MD'),\n",
       " Row(state='ME'),\n",
       " Row(state='MI'),\n",
       " Row(state='MN'),\n",
       " Row(state='MO'),\n",
       " Row(state='MS'),\n",
       " Row(state='MT'),\n",
       " Row(state='NC'),\n",
       " Row(state='ND'),\n",
       " Row(state='NE'),\n",
       " Row(state='NH'),\n",
       " Row(state='NJ'),\n",
       " Row(state='NM'),\n",
       " Row(state='NV'),\n",
       " Row(state='NY'),\n",
       " Row(state='OH'),\n",
       " Row(state='OK'),\n",
       " Row(state='OR'),\n",
       " Row(state='PA'),\n",
       " Row(state='RI'),\n",
       " Row(state='SC'),\n",
       " Row(state='SD'),\n",
       " Row(state='TN'),\n",
       " Row(state='TX'),\n",
       " Row(state='UT'),\n",
       " Row(state='VA'),\n",
       " Row(state='VT'),\n",
       " Row(state='WA'),\n",
       " Row(state='WI'),\n",
       " Row(state='WV'),\n",
       " Row(state='WY')]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_fintech_df.select(\"state\").distinct().sort(\"state\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df: pyspark.sql.dataframe.DataFrame, columns: List[str]) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply label encoding to the specified categorical columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    columns (list): List of column names to label encode.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with label-encoded columns.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    lookup_data = []\n",
    "    for column in columns:\n",
    "        state_values = df.select(column).distinct().sort(column).collect()\n",
    "        for i, row in enumerate(state_values):\n",
    "            value = row[0]\n",
    "            df = df.withColumn(column, fn.when(df[column] == value, i).otherwise(df[column]))\n",
    "            lookup_data.append(Row(original_column=column, original_value=value, encoded_column=i))\n",
    "        df = df.withColumn(column, df[column].cast(\"int\"))\n",
    "    print(lookup_data)\n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(original_column='state', original_value='AK', encoded_column=0), Row(original_column='state', original_value='AL', encoded_column=1), Row(original_column='state', original_value='AR', encoded_column=2), Row(original_column='state', original_value='AZ', encoded_column=3), Row(original_column='state', original_value='CA', encoded_column=4), Row(original_column='state', original_value='CO', encoded_column=5), Row(original_column='state', original_value='CT', encoded_column=6), Row(original_column='state', original_value='DC', encoded_column=7), Row(original_column='state', original_value='DE', encoded_column=8), Row(original_column='state', original_value='FL', encoded_column=9), Row(original_column='state', original_value='GA', encoded_column=10), Row(original_column='state', original_value='HI', encoded_column=11), Row(original_column='state', original_value='ID', encoded_column=12), Row(original_column='state', original_value='IL', encoded_column=13), Row(original_column='state', original_value='IN', encoded_column=14), Row(original_column='state', original_value='KS', encoded_column=15), Row(original_column='state', original_value='KY', encoded_column=16), Row(original_column='state', original_value='LA', encoded_column=17), Row(original_column='state', original_value='MA', encoded_column=18), Row(original_column='state', original_value='MD', encoded_column=19), Row(original_column='state', original_value='ME', encoded_column=20), Row(original_column='state', original_value='MI', encoded_column=21), Row(original_column='state', original_value='MN', encoded_column=22), Row(original_column='state', original_value='MO', encoded_column=23), Row(original_column='state', original_value='MS', encoded_column=24), Row(original_column='state', original_value='MT', encoded_column=25), Row(original_column='state', original_value='NC', encoded_column=26), Row(original_column='state', original_value='ND', encoded_column=27), Row(original_column='state', original_value='NE', encoded_column=28), Row(original_column='state', original_value='NH', encoded_column=29), Row(original_column='state', original_value='NJ', encoded_column=30), Row(original_column='state', original_value='NM', encoded_column=31), Row(original_column='state', original_value='NV', encoded_column=32), Row(original_column='state', original_value='NY', encoded_column=33), Row(original_column='state', original_value='OH', encoded_column=34), Row(original_column='state', original_value='OK', encoded_column=35), Row(original_column='state', original_value='OR', encoded_column=36), Row(original_column='state', original_value='PA', encoded_column=37), Row(original_column='state', original_value='RI', encoded_column=38), Row(original_column='state', original_value='SC', encoded_column=39), Row(original_column='state', original_value='SD', encoded_column=40), Row(original_column='state', original_value='TN', encoded_column=41), Row(original_column='state', original_value='TX', encoded_column=42), Row(original_column='state', original_value='UT', encoded_column=43), Row(original_column='state', original_value='VA', encoded_column=44), Row(original_column='state', original_value='VT', encoded_column=45), Row(original_column='state', original_value='WA', encoded_column=46), Row(original_column='state', original_value='WI', encoded_column=47), Row(original_column='state', original_value='WV', encoded_column=48), Row(original_column='state', original_value='WY', encoded_column=49), Row(original_column='purpose', original_value='car', encoded_column=0), Row(original_column='purpose', original_value='credit_card', encoded_column=1), Row(original_column='purpose', original_value='debt_consolidation', encoded_column=2), Row(original_column='purpose', original_value='home_improvement', encoded_column=3), Row(original_column='purpose', original_value='house', encoded_column=4), Row(original_column='purpose', original_value='major_purchase', encoded_column=5), Row(original_column='purpose', original_value='medical', encoded_column=6), Row(original_column='purpose', original_value='moving', encoded_column=7), Row(original_column='purpose', original_value='other', encoded_column=8), Row(original_column='purpose', original_value='renewable_energy', encoded_column=9), Row(original_column='purpose', original_value='small_business', encoded_column=10), Row(original_column='purpose', original_value='vacation', encoded_column=11), Row(original_column='purpose', original_value='wedding', encoded_column=12)]\n"
     ]
    }
   ],
   "source": [
    "label_columns = [\"state\", \"purpose\"]\n",
    "label_fintech_df = label_encode(ohe_fintech_df, label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_fintech_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "|   16|\n",
      "|   17|\n",
      "|   18|\n",
      "|   19|\n",
      "|   20|\n",
      "|   21|\n",
      "|   22|\n",
      "|   23|\n",
      "|   24|\n",
      "|   25|\n",
      "|   26|\n",
      "|   27|\n",
      "|   28|\n",
      "|   29|\n",
      "|   30|\n",
      "|   31|\n",
      "|   32|\n",
      "|   33|\n",
      "|   34|\n",
      "|   35|\n",
      "|   36|\n",
      "|   37|\n",
      "|   38|\n",
      "|   39|\n",
      "|   40|\n",
      "|   41|\n",
      "|   42|\n",
      "|   43|\n",
      "|   44|\n",
      "|   45|\n",
      "|   46|\n",
      "|   47|\n",
      "|   48|\n",
      "|   49|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_fintech_df.select(\"state\").distinct().sort('state').show(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A (1-5)\n",
    "- B (6-10)\n",
    "- C (11-15)\n",
    "- D (16-20)\n",
    "- E (21-25)\n",
    "- F (26-30)\n",
    "- G (31-35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_letter_grade(df: pyspark.sql.dataframe.DataFrame, grade_column: str=\"grade\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column `letter_grade` based on the numerical range of `grade`.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    grade_column (str): Name of the column containing numerical grades.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with an additional `letter_grade` column.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "    lookup_data = []\n",
    "    for i, letter in enumerate(letters):\n",
    "        for j in range((i * 5)+1, ((i + 1) * 5)+1):\n",
    "            lookup_data.append(Row(original_column=grade_column, original_value=str(j), encoded_value=letter))\n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    print(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "\n",
    "    \n",
    "    grade_mapping = [\n",
    "        (fn.col(grade_column).between(1, 5), \"A\"),\n",
    "        (fn.col(grade_column).between(6, 10), \"B\"),\n",
    "        (fn.col(grade_column).between(11, 15), \"C\"),\n",
    "        (fn.col(grade_column).between(16, 20), \"D\"),\n",
    "        (fn.col(grade_column).between(21, 25), \"E\"),\n",
    "        (fn.col(grade_column).between(26, 30), \"F\"),\n",
    "        (fn.col(grade_column).between(31, 35), \"G\"),\n",
    "    ]\n",
    "    \n",
    "    letter_grade_column = fn.when(*grade_mapping[0])\n",
    "    for condition, letter in grade_mapping[1:]:\n",
    "        letter_grade_column = letter_grade_column.when(condition, letter)\n",
    "    letter_grade_column = letter_grade_column.otherwise(\"Unknown\")\n",
    "\n",
    "    df = df.withColumn(\"letter_grade\", letter_grade_column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(original_column='grade', original_value='1', encoded_value='A'), Row(original_column='grade', original_value='2', encoded_value='A'), Row(original_column='grade', original_value='3', encoded_value='A'), Row(original_column='grade', original_value='4', encoded_value='A'), Row(original_column='grade', original_value='5', encoded_value='A'), Row(original_column='grade', original_value='6', encoded_value='B'), Row(original_column='grade', original_value='7', encoded_value='B'), Row(original_column='grade', original_value='8', encoded_value='B'), Row(original_column='grade', original_value='9', encoded_value='B'), Row(original_column='grade', original_value='10', encoded_value='B'), Row(original_column='grade', original_value='11', encoded_value='C'), Row(original_column='grade', original_value='12', encoded_value='C'), Row(original_column='grade', original_value='13', encoded_value='C'), Row(original_column='grade', original_value='14', encoded_value='C'), Row(original_column='grade', original_value='15', encoded_value='C'), Row(original_column='grade', original_value='16', encoded_value='D'), Row(original_column='grade', original_value='17', encoded_value='D'), Row(original_column='grade', original_value='18', encoded_value='D'), Row(original_column='grade', original_value='19', encoded_value='D'), Row(original_column='grade', original_value='20', encoded_value='D'), Row(original_column='grade', original_value='21', encoded_value='E'), Row(original_column='grade', original_value='22', encoded_value='E'), Row(original_column='grade', original_value='23', encoded_value='E'), Row(original_column='grade', original_value='24', encoded_value='E'), Row(original_column='grade', original_value='25', encoded_value='E'), Row(original_column='grade', original_value='26', encoded_value='F'), Row(original_column='grade', original_value='27', encoded_value='F'), Row(original_column='grade', original_value='28', encoded_value='F'), Row(original_column='grade', original_value='29', encoded_value='F'), Row(original_column='grade', original_value='30', encoded_value='F'), Row(original_column='grade', original_value='31', encoded_value='G'), Row(original_column='grade', original_value='32', encoded_value='G'), Row(original_column='grade', original_value='33', encoded_value='G'), Row(original_column='grade', original_value='34', encoded_value='G'), Row(original_column='grade', original_value='35', encoded_value='G')]\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df = create_letter_grade(label_fintech_df, \"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|grade|letter_grade|\n",
      "+-----+------------+\n",
      "|    1|           A|\n",
      "|    2|           A|\n",
      "|    3|           A|\n",
      "|    4|           A|\n",
      "|    5|           A|\n",
      "|    6|           B|\n",
      "|    7|           B|\n",
      "|    8|           B|\n",
      "|    9|           B|\n",
      "|   10|           B|\n",
      "|   11|           C|\n",
      "|   12|           C|\n",
      "|   13|           C|\n",
      "|   14|           C|\n",
      "|   15|           C|\n",
      "|   16|           D|\n",
      "|   17|           D|\n",
      "|   18|           D|\n",
      "|   19|           D|\n",
      "|   20|           D|\n",
      "|   21|           E|\n",
      "|   22|           E|\n",
      "|   23|           E|\n",
      "|   24|           E|\n",
      "|   25|           E|\n",
      "|   26|           F|\n",
      "|   27|           F|\n",
      "|   28|           F|\n",
      "|   29|           F|\n",
      "|   30|           F|\n",
      "|   31|           G|\n",
      "|   32|           G|\n",
      "|   33|           G|\n",
      "|   34|           G|\n",
      "|   35|           G|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df.select(\"grade\", \"letter_grade\").distinct().sort('grade').show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|emp_length|\n",
      "+----------+\n",
      "|   5 years|\n",
      "|   9 years|\n",
      "|    1 year|\n",
      "|   2 years|\n",
      "|   7 years|\n",
      "|   8 years|\n",
      "|   4 years|\n",
      "|   6 years|\n",
      "|   3 years|\n",
      "| 10+ years|\n",
      "|  < 1 year|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df.select(\"emp_length\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|emp_length|count|\n",
      "+----------+-----+\n",
      "|   5 years| 1603|\n",
      "|   9 years|  971|\n",
      "|    1 year| 1795|\n",
      "|   2 years| 2364|\n",
      "|   7 years| 1146|\n",
      "|   8 years| 1042|\n",
      "|   4 years| 1608|\n",
      "|   6 years| 1142|\n",
      "|   3 years| 2212|\n",
      "| 10+ years|10687|\n",
      "|  < 1 year| 2460|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df.groupBy(\"emp_length\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "def convert_emp_length_to_numeric(df: pyspark.sql.dataframe.DataFrame, column: str = \"emp_length\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'emp_length' column to numeric values using string manipulation.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    column (str): Column name containing employment length data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with 'emp_length' column converted to numeric values.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(column, fn.regexp_replace(fn.col(column), \"years|year\", \"\"))\n",
    "    df = df.withColumn(column, fn.when(fn.col(column).like(\"%<%\"), \"0.5\")\n",
    "                                 .when(fn.col(column).like(\"%+%\"), \"11\")\n",
    "                                 .otherwise(fn.col(column)))\n",
    "    df = df.withColumn(column, fn.col(column).cast(\"float\"))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_fintech_df_2 = convert_emp_length_to_numeric(encoded_fintech_df, \"emp_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|emp_length|count|\n",
      "+----------+-----+\n",
      "|       9.0|  971|\n",
      "|       5.0| 1603|\n",
      "|      11.0|10687|\n",
      "|       7.0| 1146|\n",
      "|       2.0| 2364|\n",
      "|       3.0| 2212|\n",
      "|       0.5| 2460|\n",
      "|       1.0| 1795|\n",
      "|       6.0| 1142|\n",
      "|       8.0| 1042|\n",
      "|       4.0| 1608|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df_2.groupBy(\"emp_length\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|emp_length|\n",
      "+----------+\n",
      "|       0.5|\n",
      "|       1.0|\n",
      "|       2.0|\n",
      "|       3.0|\n",
      "|       4.0|\n",
      "|       5.0|\n",
      "|       6.0|\n",
      "|       7.0|\n",
      "|       8.0|\n",
      "|       9.0|\n",
      "|      11.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df_2.select(\"emp_length\").distinct().sort(\"emp_length\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+\n",
      "|         customer_id|           emp_title|emp_length|annual_inc|annual_inc_joint|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|    loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|       issue_date|pymnt_plan|purpose|         description|home_ownership_own|home_ownership_any|home_ownership_mortgage|home_ownership_rent|verification_status_not verified|verification_status_verified|verification_status_source verified|type_joint|type_direct_pay|type_joint app|type_individual|letter_grade|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+\n",
      "|YicgXHgxOVx4ODU+X...| bartender/ waitress|       3.0|   35000.0|             0.0|   191xx|        PA|     2368.0|    16578.0| 139672|     Fully Paid|    14000.0|   37|      14000.0| 36 months|  0.1999|   19|    17 March 2017|     false|      5|      Major purchase|                 0|                 0|                      0|                  1|                               0|                           0|                                  1|         0|              0|             0|              1|           D|\n",
      "|YidJXHhiOFx4OWRce...|      Line- Operator|       0.5|   65000.0|             0.0|   458xx|        OH|     1263.0|     3789.0| 207602|In Grace Period|    21275.0|   34|      21275.0| 60 months|   0.124|    6|     19 July 2019|     false|      5|      Major purchase|                 1|                 0|                      0|                  0|                               0|                           0|                                  1|         0|              0|             0|              1|           B|\n",
      "|YidceGQwXHhmNl1ce...|             Teacher|       0.5|  100000.0|             0.0|   152xx|        PA|    26234.0|   341037.0| 230358|     Fully Paid|    25350.0|   37|      25350.0| 36 months|  0.1727|   19|    16 April 2016|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               0|                           1|                                  0|         0|              0|             0|              1|           D|\n",
      "|Yid6XHhkMlx4YmR8e...|   Corporate Trainer|       5.0|   41000.0|             0.0|   322xx|        FL|     3152.0|    18912.0|  98856|        Current|    10000.0|    9|      10000.0| 36 months|     0.2|   16| 18 February 2018|     false|      3|    Home improvement|                 0|                 0|                      1|                  0|                               0|                           0|                                  1|         0|              0|             0|              1|           D|\n",
      "|Yid8XHhkZlpvdFJce...|Certified Applicator|       7.0|   30000.0|             0.0|   788xx|        TX|     2451.0|    19607.0|  33846|     Fully Paid|     5000.0|   42|       5000.0| 36 months|  0.1899|   17| 17 February 2017|     false|      2|  Debt consolidation|                 0|                 0|                      0|                  1|                               0|                           1|                                  0|         0|              0|             0|              1|           D|\n",
      "|YidceDk5XHhmY1x4M...|Associate Researcher|       4.0|   67000.0|             0.0|   598xx|        MT|     6283.0|    31415.0|  21295|     Fully Paid|     4250.0|   25|       4250.0| 36 months|  0.1561|   20|    15 April 2015|     false|      2|  Debt consolidation|                 1|                 0|                      0|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           D|\n",
      "|YidceDBiQlx4OWVce...|               sales|      11.0|   60000.0|             0.0|   330xx|        FL|     1487.0|    19336.0| 167841|    Charged Off|    16000.0|    9|      16000.0| 60 months|  0.1431|   14|  15 January 2015|     false|      2|  Debt consolidation|                 0|                 0|                      0|                  1|                               0|                           0|                                  1|         0|              0|             0|              1|           C|\n",
      "|Yid1Nlx4OTRceGQ1X...|Utility Plant  En...|      11.0|   82000.0|             0.0|   088xx|        NJ|    30457.0|   335023.0| 235327|     Fully Paid|    28000.0|   30|      28000.0| 36 months|  0.0789|    2|   15 August 2015|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           A|\n",
      "|Yic5XHgxM1x4ZGRce...|      Office Manager|       0.5|  125000.0|             0.0|   346xx|        FL|     8382.0|   125734.0| 193947|        Current|    20000.0|    9|      20000.0| 36 months|  0.1349|   15|16 September 2016|     false|      3|    Home improvement|                 0|                 0|                      1|                  0|                               0|                           0|                                  1|         0|              0|             0|              1|           C|\n",
      "|YidceGNiPFZ3XHhkN...|       Sales Manager|       2.0|   40000.0|             0.0|   606xx|        IL|     2656.0|    10624.0|  95200|     Fully Paid|    10000.0|   13|      10000.0| 36 months|  0.1398|   12| 13 December 2013|     false|      2|Good Bet Debt Con...|                 0|                 0|                      0|                  1|                               1|                           0|                                  0|         0|              0|             0|              1|           C|\n",
      "|YidceGZkOFx4ZTZce...|Logistics coordin...|       3.0|   47800.0|             0.0|   902xx|        CA|     2855.0|    14276.0|  40657|        Current|     6000.0|    4|       6000.0| 36 months|  0.0797|    3|      17 May 2017|     false|      1|Credit card refin...|                 0|                 0|                      0|                  1|                               0|                           1|                                  0|         0|              0|             0|              1|           A|\n",
      "|YidceDE3TzQuXHhhY...|           Echo tech|       0.5|   64999.0|             0.0|   481xx|        MI|     4442.0|   133252.0|  20325|        Current|     4050.0|   21|       4050.0| 36 months|  0.1102|   10| 19 November 2019|     false|      1|Credit card refin...|                 0|                 0|                      0|                  1|                               1|                           0|                                  0|         0|              0|             0|              1|           B|\n",
      "|YidLXHhjMFx4ZDVce...|      LOAN PROCESSOR|      11.0|   75000.0|             0.0|   365xx|        AL|    12223.0|   293351.0| 158526|        Current|    15000.0|    1|      15000.0| 60 months|  0.1953|   20|    16 April 2016|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               0|                           1|                                  0|         0|              0|             0|              1|           D|\n",
      "|YicoXHg5MFx4ZjFce...|       HR consultant|       8.0|   75000.0|             0.0|   840xx|        UT|    16220.0|   243302.0| 186598|        Current|    19825.0|   43|      19825.0| 60 months|  0.2399|   29|  15 January 2015|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           F|\n",
      "|YidDXHhjMDVceDFlX...|Director of Stude...|       1.0|   71937.0|             0.0|   852xx|        AZ|     9603.0|   105631.0| 231429|        Current|    26000.0|    3|      26000.0| 36 months|  0.1499|   13|14 September 2014|     false|      2|  Debt consolidation|                 0|                 0|                      0|                  1|                               0|                           0|                                  1|         0|              0|             0|              1|           C|\n",
      "|YidceGJjXHhiNVx4Y...|    Building Manager|      11.0|   51000.0|             0.0|   801xx|        CO|     3237.0|    16184.0|   1664|        Current|     1200.0|    5|       1200.0| 36 months|  0.1942|   16|     18 June 2018|     false|      2|  Debt consolidation|                 0|                 0|                      0|                  1|                               1|                           0|                                  0|         0|              0|             0|              1|           D|\n",
      "|YidceGFkXHg4ZVx4O...|Human Resource Ma...|       9.0|   58000.0|             0.0|   146xx|        NY|     7212.0|   122608.0|  70546|        Current|     8400.0|   33|       8400.0| 36 months|  0.1049|    7| 16 November 2016|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           B|\n",
      "|Yiczflx4YjUqXHhlN...|             Teacher|       0.5|  120000.0|             0.0|   891xx|        NV|    20113.0|   201125.0| 253983|     Fully Paid|    35000.0|   32|      35000.0| 36 months|  0.0724|    1|  17 January 2017|     false|      3|    Home improvement|                 0|                 0|                      1|                  0|                               0|                           1|                                  0|         0|              0|             0|              1|           A|\n",
      "|YiJyJ1x4ODZceDAwX...| Deportation officer|      11.0|  115000.0|             0.0|   919xx|        CA|    66538.0|   598839.0| 243836|     Fully Paid|    30000.0|    4|      30000.0| 60 months|  0.1008|    6|18 September 2018|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               0|                           1|                                  0|         0|              0|             0|              1|           B|\n",
      "|YicrXHgxOFx4Y2Nce...|Internet Sales Co...|       0.5|   37000.0|             0.0|   805xx|        CO|    22918.0|   206261.0|  86629|     Fully Paid|    10000.0|    5|      10000.0| 36 months|  0.0818|    7|  15 October 2015|     false|      1|Credit card refin...|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           B|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|    original_column| original_value|      encoded_column|\n",
      "+-------------------+---------------+--------------------+\n",
      "|     home_ownership|            own|  home_ownership_own|\n",
      "|     home_ownership|            any|  home_ownership_any|\n",
      "|     home_ownership|       mortgage|home_ownership_mo...|\n",
      "|     home_ownership|           rent| home_ownership_rent|\n",
      "|verification_status|   not verified|verification_stat...|\n",
      "|verification_status|       verified|verification_stat...|\n",
      "|verification_status|source verified|verification_stat...|\n",
      "|               type|          joint|          type_joint|\n",
      "|               type|     direct_pay|     type_direct_pay|\n",
      "|               type|      joint app|      type_joint app|\n",
      "|               type|     individual|     type_individual|\n",
      "|              state|             AK|                   0|\n",
      "|              state|             AL|                   1|\n",
      "|              state|             AR|                   2|\n",
      "|              state|             AZ|                   3|\n",
      "|              state|             CA|                   4|\n",
      "|              state|             CO|                   5|\n",
      "|              state|             CT|                   6|\n",
      "|              state|             DC|                   7|\n",
      "|              state|             DE|                   8|\n",
      "|              state|             FL|                   9|\n",
      "|              state|             GA|                  10|\n",
      "|              state|             HI|                  11|\n",
      "|              state|             ID|                  12|\n",
      "|              state|             IL|                  13|\n",
      "|              state|             IN|                  14|\n",
      "|              state|             KS|                  15|\n",
      "|              state|             KY|                  16|\n",
      "|              state|             LA|                  17|\n",
      "|              state|             MA|                  18|\n",
      "|              state|             MD|                  19|\n",
      "|              state|             ME|                  20|\n",
      "|              state|             MI|                  21|\n",
      "|              state|             MN|                  22|\n",
      "|              state|             MO|                  23|\n",
      "|              state|             MS|                  24|\n",
      "|              state|             MT|                  25|\n",
      "|              state|             NC|                  26|\n",
      "|              state|             ND|                  27|\n",
      "|              state|             NE|                  28|\n",
      "|              state|             NH|                  29|\n",
      "|              state|             NJ|                  30|\n",
      "|              state|             NM|                  31|\n",
      "|              state|             NV|                  32|\n",
      "|              state|             NY|                  33|\n",
      "|              state|             OH|                  34|\n",
      "|              state|             OK|                  35|\n",
      "|              state|             OR|                  36|\n",
      "|              state|             PA|                  37|\n",
      "|              state|             RI|                  38|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_LOOKUP_TABLE.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that adds the 3 following features. Try as much as you can to use\n",
    "built in fucntions in PySpark (from the functions library) check lab 8, Avoid writing\n",
    "UDFs from scratch.\n",
    "- Previous loan issue date from the same grade\n",
    "- Previoius Loan amount from the same grade\n",
    "- Previous loan date from the same state and grade combined\n",
    "- Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan issue date from the same grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous_loan_issue_date_form_same_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_issue_date_same_grade' that contains the issue date of the previous loan with the same grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_issue_date_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_issue_date = fn.lag(\"issue_date\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_issue_date_same_grade\", prev_issue_date)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df = add_previous_loan_issue_date_form_same_grade(encoded_fintech_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------------------------+\n",
      "|       issue_date|grade|previous_loan_issue_date_same_grade|\n",
      "+-----------------+-----+-----------------------------------+\n",
      "|12 September 2012|    1|                               NULL|\n",
      "|12 September 2012|    1|                  12 September 2012|\n",
      "|12 September 2012|    1|                  12 September 2012|\n",
      "|  12 October 2012|    1|                  12 September 2012|\n",
      "| 12 November 2012|    1|                    12 October 2012|\n",
      "| 12 December 2012|    1|                   12 November 2012|\n",
      "| 12 December 2012|    1|                   12 December 2012|\n",
      "|  13 January 2013|    1|                   12 December 2012|\n",
      "|  13 January 2013|    1|                    13 January 2013|\n",
      "| 13 February 2013|    1|                    13 January 2013|\n",
      "| 13 February 2013|    1|                   13 February 2013|\n",
      "|    13 March 2013|    1|                   13 February 2013|\n",
      "|    13 March 2013|    1|                      13 March 2013|\n",
      "|    13 March 2013|    1|                      13 March 2013|\n",
      "|    13 April 2013|    1|                      13 March 2013|\n",
      "|    13 April 2013|    1|                      13 April 2013|\n",
      "|    13 April 2013|    1|                      13 April 2013|\n",
      "|      13 May 2013|    1|                      13 April 2013|\n",
      "|      13 May 2013|    1|                        13 May 2013|\n",
      "|     13 June 2013|    1|                        13 May 2013|\n",
      "+-----------------+-----+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df.select(\"issue_date\", \"grade\", \"previous_loan_issue_date_same_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previoius Loan amount from the same grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_amount_from_same_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_amount_same_grade' that contains the loan amount of the previous loan with the same grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"loan_amount\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_amount_same_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_2 = add_prev_loan_amount_from_same_grade(lagged_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----+-------------------------------+\n",
      "|       issue_date|loan_amount|grade|previous_loan_amount_same_grade|\n",
      "+-----------------+-----------+-----+-------------------------------+\n",
      "|12 September 2012|    12000.0|    1|                           NULL|\n",
      "|12 September 2012|    11200.0|    1|                        12000.0|\n",
      "|12 September 2012|     6500.0|    1|                        11200.0|\n",
      "|  12 October 2012|     8000.0|    1|                         6500.0|\n",
      "| 12 November 2012|    10000.0|    1|                         8000.0|\n",
      "| 12 December 2012|    24000.0|    1|                        10000.0|\n",
      "| 12 December 2012|    24000.0|    1|                        24000.0|\n",
      "|  13 January 2013|    16850.0|    1|                        24000.0|\n",
      "|  13 January 2013|    12000.0|    1|                        16850.0|\n",
      "| 13 February 2013|    10000.0|    1|                        12000.0|\n",
      "| 13 February 2013|    28000.0|    1|                        10000.0|\n",
      "|    13 March 2013|    21000.0|    1|                        28000.0|\n",
      "|    13 March 2013|    16000.0|    1|                        21000.0|\n",
      "|    13 March 2013|    12500.0|    1|                        16000.0|\n",
      "|    13 April 2013|    17400.0|    1|                        12500.0|\n",
      "|    13 April 2013|    24000.0|    1|                        17400.0|\n",
      "|    13 April 2013|    14000.0|    1|                        24000.0|\n",
      "|      13 May 2013|    14400.0|    1|                        14000.0|\n",
      "|      13 May 2013|    24000.0|    1|                        14400.0|\n",
      "|     13 June 2013|     6000.0|    1|                        24000.0|\n",
      "+-----------------+-----------+-----+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_2.select(\"issue_date\",\"loan_amount\", \"grade\", \"previous_loan_amount_same_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan date from the same state and grade combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_date_from_same_state_and_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_date_same_state_and_grade' that contains the issue date of the previous loan with the same state and grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"state\",\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"issue_date\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_date_same_state_and_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_3 = add_prev_loan_date_from_same_state_and_grade(lagged_fintech_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+---------------------------------------+\n",
      "|       issue_date|state|grade|previous_loan_date_same_state_and_grade|\n",
      "+-----------------+-----+-----+---------------------------------------+\n",
      "|  14 October 2014|    0|    1|                                   NULL|\n",
      "|  19 January 2019|    0|    1|                        14 October 2014|\n",
      "| 19 November 2019|    0|    1|                        19 January 2019|\n",
      "|  14 October 2014|    0|    2|                                   NULL|\n",
      "|     17 June 2017|    0|    2|                        14 October 2014|\n",
      "| 17 November 2017|    0|    2|                           17 June 2017|\n",
      "|    18 March 2018|    0|    2|                       17 November 2017|\n",
      "|15 September 2015|    0|    3|                                   NULL|\n",
      "|  18 October 2018|    0|    3|                      15 September 2015|\n",
      "|    19 April 2019|    0|    3|                        18 October 2018|\n",
      "|     16 June 2016|    0|    4|                                   NULL|\n",
      "|      17 May 2017|    0|    4|                           16 June 2016|\n",
      "| 14 November 2014|    0|    5|                                   NULL|\n",
      "|  15 January 2015|    0|    5|                       14 November 2014|\n",
      "|  18 October 2018|    0|    5|                        15 January 2015|\n",
      "|  18 October 2018|    0|    5|                        18 October 2018|\n",
      "|     17 July 2017|    0|    6|                                   NULL|\n",
      "|     16 July 2016|    0|    7|                                   NULL|\n",
      "|   16 August 2016|    0|    7|                           16 July 2016|\n",
      "| 19 February 2019|    0|    7|                         16 August 2016|\n",
      "+-----------------+-----+-----+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_3.select(\"issue_date\",\"state\",\"grade\", \"previous_loan_date_same_state_and_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_amount_from_same_state_and_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_amount_same_state_and_grade' that contains the loan amount of the previous loan with the same state and grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_state_and_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"state\",\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"loan_amount\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_amount_same_state_and_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_4 = add_prev_loan_amount_from_same_state_and_grade(lagged_fintech_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "|       issue_date|state|loan_amount|grade|previous_loan_amount_same_state_and_grade|\n",
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "|  14 October 2014|    0|    10000.0|    1|                                     NULL|\n",
      "|  19 January 2019|    0|    10000.0|    1|                                  10000.0|\n",
      "| 19 November 2019|    0|     6000.0|    1|                                  10000.0|\n",
      "|  14 October 2014|    0|    24000.0|    2|                                     NULL|\n",
      "|     17 June 2017|    0|    30000.0|    2|                                  24000.0|\n",
      "| 17 November 2017|    0|    21375.0|    2|                                  30000.0|\n",
      "|    18 March 2018|    0|    35000.0|    2|                                  21375.0|\n",
      "|15 September 2015|    0|    15000.0|    3|                                     NULL|\n",
      "|  18 October 2018|    0|    25000.0|    3|                                  15000.0|\n",
      "|    19 April 2019|    0|    40000.0|    3|                                  25000.0|\n",
      "|     16 June 2016|    0|    15000.0|    4|                                     NULL|\n",
      "|      17 May 2017|    0|    16000.0|    4|                                  15000.0|\n",
      "| 14 November 2014|    0|    12000.0|    5|                                     NULL|\n",
      "|  15 January 2015|    0|    25000.0|    5|                                  12000.0|\n",
      "|  18 October 2018|    0|    32000.0|    5|                                  25000.0|\n",
      "|  18 October 2018|    0|    33600.0|    5|                                  32000.0|\n",
      "|     17 July 2017|    0|    11000.0|    6|                                     NULL|\n",
      "|     16 July 2016|    0|    12000.0|    7|                                     NULL|\n",
      "|   16 August 2016|    0|    12000.0|    7|                                  12000.0|\n",
      "| 19 February 2019|    0|    10000.0|    7|                                  12000.0|\n",
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_4.select(\"issue_date\",\"state\",\"loan_amount\", \"grade\", \"previous_loan_amount_same_state_and_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Analysis SQL vs Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer each of the following questions using both SQL and Spark:\n",
    "1. Identify the average loan amount and interest rate for loans marked as\n",
    "\"Default\" in the Loan Status, grouped by Emp Length and annual income ranges.\n",
    "Hint: Use SQL Cases to bin Annual Income into Income Ranges\n",
    "2. Calculate the average difference between Loan Amount and Funded Amount for each\n",
    "loan Grade and sort by the grades with the largest differences.\n",
    "3. Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\"\n",
    "Verification Status across each state (Addr State).\n",
    "4. Calculate the average time gap (in days) between consecutive loans for each\n",
    "grade using the new features you added in the feature engineering phase\n",
    "5. Identify the average difference in loan amounts between consecutive loans\n",
    "within the same state and grade combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_4.createOrReplaceTempView(\"fintech_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equality_of_dfs(df1: pyspark.sql.dataframe.DataFrame, df2: pyspark.sql.dataframe.DataFrame) -> bool:\n",
    "    if df1.schema != df2.schema:\n",
    "        print(\"DataFrames have different schemas.\")\n",
    "        return False\n",
    "    \n",
    "    if df1.exceptAll(df2).isEmpty() and df2.exceptAll(df1).isEmpty():\n",
    "        print(\"The DataFrames are exactly equal.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The DataFrames are not exactly equal.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Identify the average loan amount and interest rate for loans marked as \"Default\" in the Loan Status, grouped by Emp Length and annual income ranges.\n",
    "\n",
    "Hint: Use SQL Cases to bin Annual Income into Income Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT \n",
    "    AVG(int_rate) AS avg_int_rate, \n",
    "    AVG(loan_amount) AS avg_loan_amount, \n",
    "    emp_length, \n",
    "    CASE \n",
    "        WHEN annual_inc < 50000 THEN 'Low'\n",
    "        WHEN annual_inc BETWEEN 50000 AND 200000 THEN 'Medium'\n",
    "        ELSE 'High'\n",
    "    END AS annual_inc_category\n",
    "FROM fintech_data\n",
    "WHERE loan_status = 'Default'\n",
    "GROUP BY emp_length, annual_inc_category\n",
    "\"\"\"\n",
    "\n",
    "q1_sql_df = spark.sql(query_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------+-------------------+\n",
      "|avg_int_rate|avg_loan_amount|emp_length|annual_inc_category|\n",
      "+------------+---------------+----------+-------------------+\n",
      "|      0.0789|        11900.0|       6.0|                Low|\n",
      "|      0.2075|        15000.0|       1.0|             Medium|\n",
      "|      0.2589|        35000.0|       1.0|               High|\n",
      "+------------+---------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------+-------------------+\n",
      "|avg_int_rate|avg_loan_amount|emp_length|annual_inc_category|\n",
      "+------------+---------------+----------+-------------------+\n",
      "|      0.0789|        11900.0|       6.0|                Low|\n",
      "|      0.2075|        15000.0|       1.0|             Medium|\n",
      "|      0.2589|        35000.0|       1.0|               High|\n",
      "+------------+---------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "default_loans_df = lagged_fintech_df_4.filter(fn.col(\"loan_status\") == \"Default\")\n",
    "\n",
    "categorized_loans_df = default_loans_df.withColumn(\n",
    "    \"annual_inc_category\",\n",
    "    fn.when(fn.col(\"annual_inc\") < 50000, \"Low\")\n",
    "    .when((fn.col(\"annual_inc\") >= 50000) & (fn.col(\"annual_inc\") <= 200000), \"Medium\")\n",
    "    .otherwise(\"High\"),\n",
    ")\n",
    "\n",
    "grouped_loans_df = categorized_loans_df.groupBy(\"emp_length\", \"annual_inc_category\")\n",
    "\n",
    "q1_spark_df = grouped_loans_df.agg(\n",
    "    fn.avg(\"loan_amount\").alias(\"avg_loan_amount\"),\n",
    "    fn.avg(\"int_rate\").alias(\"avg_int_rate\"),\n",
    ")\n",
    "\n",
    "q1_spark_df = q1_spark_df.select(\n",
    "    \"avg_int_rate\", \n",
    "    \"avg_loan_amount\", \n",
    "    \"emp_length\", \n",
    "    \"annual_inc_category\"\n",
    ")\n",
    "\n",
    "q1_spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are exactly equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_dfs(q1_sql_df, q1_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Calculate the average difference between Loan Amount and Funded Amount for each\n",
    "loan Grade and sort by the grades with the largest differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|grade|avg_amount_difference|\n",
      "+-----+---------------------+\n",
      "|1    |0.0                  |\n",
      "|2    |0.0                  |\n",
      "|3    |0.0                  |\n",
      "|4    |0.0                  |\n",
      "|5    |0.0                  |\n",
      "|6    |0.0                  |\n",
      "|7    |0.0                  |\n",
      "|8    |0.0                  |\n",
      "|9    |0.0                  |\n",
      "|10   |0.0                  |\n",
      "|11   |0.0                  |\n",
      "|12   |0.0                  |\n",
      "|13   |0.0                  |\n",
      "|14   |0.0                  |\n",
      "|15   |0.0                  |\n",
      "|16   |0.0                  |\n",
      "|17   |0.0                  |\n",
      "|18   |0.0                  |\n",
      "|19   |0.0                  |\n",
      "|20   |0.0                  |\n",
      "|21   |0.0                  |\n",
      "|22   |0.0                  |\n",
      "|23   |0.0                  |\n",
      "|24   |0.0                  |\n",
      "|25   |0.0                  |\n",
      "|26   |0.0                  |\n",
      "|27   |0.0                  |\n",
      "|28   |0.0                  |\n",
      "|29   |0.0                  |\n",
      "|30   |0.0                  |\n",
      "|31   |0.0                  |\n",
      "|32   |0.0                  |\n",
      "|33   |0.0                  |\n",
      "|34   |0.0                  |\n",
      "|35   |0.0                  |\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT \n",
    "    grade,\n",
    "    AVG(loan_amount - funded_amount) AS avg_amount_difference\n",
    "FROM fintech_data\n",
    "GROUP BY grade\n",
    "ORDER BY avg_amount_difference DESC, grade ASC\n",
    "\"\"\"\n",
    "\n",
    "q2_sql_df = spark.sql(query_2)\n",
    "q2_sql_df.show(q2_sql_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|grade|avg_amount_difference|\n",
      "+-----+---------------------+\n",
      "|1    |0.0                  |\n",
      "|2    |0.0                  |\n",
      "|3    |0.0                  |\n",
      "|4    |0.0                  |\n",
      "|5    |0.0                  |\n",
      "|6    |0.0                  |\n",
      "|7    |0.0                  |\n",
      "|8    |0.0                  |\n",
      "|9    |0.0                  |\n",
      "|10   |0.0                  |\n",
      "|11   |0.0                  |\n",
      "|12   |0.0                  |\n",
      "|13   |0.0                  |\n",
      "|14   |0.0                  |\n",
      "|15   |0.0                  |\n",
      "|16   |0.0                  |\n",
      "|17   |0.0                  |\n",
      "|18   |0.0                  |\n",
      "|19   |0.0                  |\n",
      "|20   |0.0                  |\n",
      "|21   |0.0                  |\n",
      "|22   |0.0                  |\n",
      "|23   |0.0                  |\n",
      "|24   |0.0                  |\n",
      "|25   |0.0                  |\n",
      "|26   |0.0                  |\n",
      "|27   |0.0                  |\n",
      "|28   |0.0                  |\n",
      "|29   |0.0                  |\n",
      "|30   |0.0                  |\n",
      "|31   |0.0                  |\n",
      "|32   |0.0                  |\n",
      "|33   |0.0                  |\n",
      "|34   |0.0                  |\n",
      "|35   |0.0                  |\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_amount_diff = fn.col('loan_amount') - fn.col('funded_amount')\n",
    "grouped_by_grade_df = lagged_fintech_df_4.groupBy('grade')\n",
    "q2_spark_df = grouped_by_grade_df \\\n",
    "    .agg(fn.avg(loan_amount_diff).alias('avg_amount_difference')) \\\n",
    "    .orderBy(fn.desc('avg_amount_difference'), fn.asc('grade'))\n",
    "q2_spark_df.show(q2_spark_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are exactly equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_dfs(q2_sql_df, q2_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\"\n",
    "Verification Status across each state (Addr State)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+------------------------------+\n",
      "|addr_state|verified_total_loan_amount|not_verified_total_loan_amount|\n",
      "+----------+--------------------------+------------------------------+\n",
      "|AZ        |2637100.0                 |3098950.0                     |\n",
      "|SC        |1627475.0                 |1752025.0                     |\n",
      "|LA        |1309425.0                 |1160100.0                     |\n",
      "|MN        |1519050.0                 |2337500.0                     |\n",
      "|NJ        |5230225.0                 |4557250.0                     |\n",
      "|DC        |156350.0                  |243925.0                      |\n",
      "|OR        |1229700.0                 |1401775.0                     |\n",
      "|VA        |3419325.0                 |3982500.0                     |\n",
      "|RI        |529125.0                  |768675.0                      |\n",
      "|KY        |1243300.0                 |1105025.0                     |\n",
      "|WY        |235825.0                  |252925.0                      |\n",
      "|NH        |638650.0                  |631100.0                      |\n",
      "|MI        |3008450.0                 |3668700.0                     |\n",
      "|NV        |1684200.0                 |1916150.0                     |\n",
      "|WI        |1576550.0                 |1686000.0                     |\n",
      "|ID        |348150.0                  |219450.0                      |\n",
      "|CA        |1.6004475E7               |1.777085E7                    |\n",
      "|CT        |1643750.0                 |1652675.0                     |\n",
      "|NE        |327800.0                  |569800.0                      |\n",
      "|NC        |3326275.0                 |3622475.0                     |\n",
      "|VT        |244075.0                  |299275.0                      |\n",
      "|MD        |3458200.0                 |2690325.0                     |\n",
      "|MO        |1731325.0                 |1876125.0                     |\n",
      "|IL        |4913050.0                 |5723475.0                     |\n",
      "|WA        |2726825.0                 |2684275.0                     |\n",
      "|MS        |628000.0                  |676875.0                      |\n",
      "|AL        |1437450.0                 |1625925.0                     |\n",
      "|IN        |2007575.0                 |1947200.0                     |\n",
      "|OH        |4443150.0                 |4370975.0                     |\n",
      "|TN        |1967825.0                 |2070575.0                     |\n",
      "|NM        |615200.0                  |745075.0                      |\n",
      "|PA        |4293975.0                 |4485800.0                     |\n",
      "|SD        |218775.0                  |398775.0                      |\n",
      "|NY        |9287375.0                 |1.1047575E7                   |\n",
      "|TX        |9724100.0                 |1.1377325E7                   |\n",
      "|WV        |545900.0                  |677000.0                      |\n",
      "|GA        |4134600.0                 |4188400.0                     |\n",
      "|MA        |2392475.0                 |3504775.0                     |\n",
      "|CO        |2270250.0                 |2838250.0                     |\n",
      "|FL        |7893600.0                 |9837075.0                     |\n",
      "|AK        |306175.0                  |350400.0                      |\n",
      "|AR        |689550.0                  |896100.0                      |\n",
      "|OK        |1057175.0                 |1436025.0                     |\n",
      "|HI        |444700.0                  |508200.0                      |\n",
      "|ND        |222500.0                  |178625.0                      |\n",
      "|KS        |1113800.0                 |926625.0                      |\n",
      "|MT        |417425.0                  |326225.0                      |\n",
      "|ME        |146875.0                  |403225.0                      |\n",
      "|UT        |701050.0                  |884800.0                      |\n",
      "|DE        |467850.0                  |319275.0                      |\n",
      "+----------+--------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_3_1 = \"\"\"\n",
    "SELECT \n",
    "    addr_state, \n",
    "    SUM(loan_amount) AS total_loan_amount\n",
    "FROM fintech_data\n",
    "WHERE verification_status_verified = 1\n",
    "GROUP BY addr_state\n",
    "\"\"\"\n",
    "spark.sql(query_3_1).createOrReplaceTempView(\"verified_total_loan_amount\")\n",
    "\n",
    "query_3_2 = \"\"\"\n",
    "SELECT \n",
    "    addr_state, \n",
    "    SUM(loan_amount) AS total_loan_amount\n",
    "FROM fintech_data\n",
    "WHERE `verification_status_not verified` = 1\n",
    "GROUP BY addr_state\n",
    "\"\"\"\n",
    "spark.sql(query_3_2).createOrReplaceTempView(\"not_verified_total_loan_amount\")\n",
    "\n",
    "\n",
    "query_3 = \"\"\"\n",
    "SELECT \n",
    "    verified_total_loan_amount.addr_state, \n",
    "    COALESCE(verified_total_loan_amount.total_loan_amount, 0) AS verified_total_loan_amount, \n",
    "    COALESCE(not_verified_total_loan_amount.total_loan_amount,0) AS not_verified_total_loan_amount\n",
    "FROM verified_total_loan_amount LEFT JOIN not_verified_total_loan_amount ON verified_total_loan_amount.addr_state = not_verified_total_loan_amount.addr_state\n",
    "\"\"\"\n",
    "q3_sql_df = spark.sql(query_3)\n",
    "\n",
    "q3_sql_df.show(q3_sql_df.count(), truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+------------------------------+\n",
      "|addr_state|verified_total_loan_amount|not_verified_total_loan_amount|\n",
      "+----------+--------------------------+------------------------------+\n",
      "|AZ        |2637100.0                 |3098950.0                     |\n",
      "|SC        |1627475.0                 |1752025.0                     |\n",
      "|LA        |1309425.0                 |1160100.0                     |\n",
      "|MN        |1519050.0                 |2337500.0                     |\n",
      "|NJ        |5230225.0                 |4557250.0                     |\n",
      "|DC        |156350.0                  |243925.0                      |\n",
      "|OR        |1229700.0                 |1401775.0                     |\n",
      "|VA        |3419325.0                 |3982500.0                     |\n",
      "|RI        |529125.0                  |768675.0                      |\n",
      "|KY        |1243300.0                 |1105025.0                     |\n",
      "|WY        |235825.0                  |252925.0                      |\n",
      "|NH        |638650.0                  |631100.0                      |\n",
      "|MI        |3008450.0                 |3668700.0                     |\n",
      "|NV        |1684200.0                 |1916150.0                     |\n",
      "|WI        |1576550.0                 |1686000.0                     |\n",
      "|ID        |348150.0                  |219450.0                      |\n",
      "|CA        |1.6004475E7               |1.777085E7                    |\n",
      "|CT        |1643750.0                 |1652675.0                     |\n",
      "|NE        |327800.0                  |569800.0                      |\n",
      "|NC        |3326275.0                 |3622475.0                     |\n",
      "|VT        |244075.0                  |299275.0                      |\n",
      "|MD        |3458200.0                 |2690325.0                     |\n",
      "|MO        |1731325.0                 |1876125.0                     |\n",
      "|IL        |4913050.0                 |5723475.0                     |\n",
      "|WA        |2726825.0                 |2684275.0                     |\n",
      "|MS        |628000.0                  |676875.0                      |\n",
      "|AL        |1437450.0                 |1625925.0                     |\n",
      "|IN        |2007575.0                 |1947200.0                     |\n",
      "|OH        |4443150.0                 |4370975.0                     |\n",
      "|TN        |1967825.0                 |2070575.0                     |\n",
      "|NM        |615200.0                  |745075.0                      |\n",
      "|PA        |4293975.0                 |4485800.0                     |\n",
      "|SD        |218775.0                  |398775.0                      |\n",
      "|NY        |9287375.0                 |1.1047575E7                   |\n",
      "|TX        |9724100.0                 |1.1377325E7                   |\n",
      "|WV        |545900.0                  |677000.0                      |\n",
      "|GA        |4134600.0                 |4188400.0                     |\n",
      "|MA        |2392475.0                 |3504775.0                     |\n",
      "|CO        |2270250.0                 |2838250.0                     |\n",
      "|FL        |7893600.0                 |9837075.0                     |\n",
      "|AK        |306175.0                  |350400.0                      |\n",
      "|AR        |689550.0                  |896100.0                      |\n",
      "|OK        |1057175.0                 |1436025.0                     |\n",
      "|HI        |444700.0                  |508200.0                      |\n",
      "|ND        |222500.0                  |178625.0                      |\n",
      "|KS        |1113800.0                 |926625.0                      |\n",
      "|MT        |417425.0                  |326225.0                      |\n",
      "|ME        |146875.0                  |403225.0                      |\n",
      "|UT        |701050.0                  |884800.0                      |\n",
      "|DE        |467850.0                  |319275.0                      |\n",
      "+----------+--------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_verified_total_loans = lagged_fintech_df_4.filter(fn.col('verification_status_verified')==1)\\\n",
    "    .groupby('addr_state')\\\n",
    "    .agg(fn.sum('loan_amount').alias('verified_total_loan_amount'))\n",
    "\n",
    "sum_not_verified_total_loans = lagged_fintech_df_4.filter(fn.col('verification_status_not verified')==1)\\\n",
    "    .groupby('addr_state')\\\n",
    "    .agg(fn.sum('loan_amount').alias('not_verified_total_loan_amount'))\n",
    "\n",
    "q3_spark_df = sum_verified_total_loans.join(sum_not_verified_total_loans, 'addr_state', 'left')\\\n",
    "    .select('addr_state', 'verified_total_loan_amount', 'not_verified_total_loan_amount')\\\n",
    "\n",
    "q3_spark_df = q3_spark_df.fillna(0)\n",
    "q3_spark_df.show(q3_spark_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are exactly equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_dfs(q3_sql_df, q3_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Calculate the average time gap (in days) between consecutive loans for each\n",
    "grade using the new features you added in the feature engineering phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|grade|avg_days_between_loans|\n",
      "+-----+----------------------+\n",
      "|1    |2.235888795282224     |\n",
      "|2    |2.2988013698630136    |\n",
      "|3    |2.2548853016142734    |\n",
      "|4    |2.273497036409822     |\n",
      "|5    |2.3368146214099217    |\n",
      "|6    |1.6947637292464879    |\n",
      "|7    |1.7606557377049181    |\n",
      "|8    |1.756452680344143     |\n",
      "|9    |1.6566791510611736    |\n",
      "|10   |1.669776119402985     |\n",
      "|11   |1.7526109660574412    |\n",
      "|12   |1.742613263296126     |\n",
      "|13   |1.6765634870499053    |\n",
      "|14   |1.742613263296126     |\n",
      "|15   |1.7716955941255006    |\n",
      "|16   |3.477979274611399     |\n",
      "|17   |3.6283783783783785    |\n",
      "|18   |3.638211382113821     |\n",
      "|19   |3.4480946123521683    |\n",
      "|20   |3.372299872935197     |\n",
      "|21   |8.338129496402878     |\n",
      "|22   |8.926923076923076     |\n",
      "|23   |9.309236947791165     |\n",
      "|24   |8.307420494699647     |\n",
      "|25   |7.685430463576159     |\n",
      "|26   |25.69736842105263     |\n",
      "|27   |30.955882352941178    |\n",
      "|28   |27.384615384615383    |\n",
      "|29   |34.04347826086956     |\n",
      "|30   |27.810126582278482    |\n",
      "|31   |64.53846153846153     |\n",
      "|32   |110.84210526315789    |\n",
      "|33   |93.0                  |\n",
      "|34   |55.096774193548384    |\n",
      "|35   |64.72                 |\n",
      "+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_4 = \"\"\"\n",
    "SELECT\n",
    "    grade,\n",
    "    AVG(DATEDIFF(\n",
    "        TO_DATE(issue_date, 'dd MMMM yyyy'),\n",
    "        TO_DATE(previous_loan_issue_date_same_grade, 'dd MMMM yyyy')\n",
    "    )) AS avg_days_between_loans\n",
    "FROM fintech_data\n",
    "WHERE previous_loan_issue_date_same_grade IS NOT NULL\n",
    "GROUP BY grade\n",
    "\"\"\"\n",
    "\n",
    "q4_sql_df = spark.sql(query_4)\n",
    "q4_sql_df.show(q4_sql_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|grade|avg_days_between_loans|\n",
      "+-----+----------------------+\n",
      "|1    |2.235888795282224     |\n",
      "|2    |2.2988013698630136    |\n",
      "|3    |2.2548853016142734    |\n",
      "|4    |2.273497036409822     |\n",
      "|5    |2.3368146214099217    |\n",
      "|6    |1.6947637292464879    |\n",
      "|7    |1.7606557377049181    |\n",
      "|8    |1.756452680344143     |\n",
      "|9    |1.6566791510611736    |\n",
      "|10   |1.669776119402985     |\n",
      "|11   |1.7526109660574412    |\n",
      "|12   |1.742613263296126     |\n",
      "|13   |1.6765634870499053    |\n",
      "|14   |1.742613263296126     |\n",
      "|15   |1.7716955941255006    |\n",
      "|16   |3.477979274611399     |\n",
      "|17   |3.6283783783783785    |\n",
      "|18   |3.638211382113821     |\n",
      "|19   |3.4480946123521683    |\n",
      "|20   |3.372299872935197     |\n",
      "|21   |8.338129496402878     |\n",
      "|22   |8.926923076923076     |\n",
      "|23   |9.309236947791165     |\n",
      "|24   |8.307420494699647     |\n",
      "|25   |7.685430463576159     |\n",
      "|26   |25.69736842105263     |\n",
      "|27   |30.955882352941178    |\n",
      "|28   |27.384615384615383    |\n",
      "|29   |34.04347826086956     |\n",
      "|30   |27.810126582278482    |\n",
      "|31   |64.53846153846153     |\n",
      "|32   |110.84210526315789    |\n",
      "|33   |93.0                  |\n",
      "|34   |55.096774193548384    |\n",
      "|35   |64.72                 |\n",
      "+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_issue_date = fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\")\n",
    "preprocessed_prev_issue_date = fn.to_date(fn.col(\"previous_loan_issue_date_same_grade\"), \"dd MMMM yyyy\")\n",
    "\n",
    "q4_spark_df = lagged_fintech_df_4.filter(fn.col(\"previous_loan_issue_date_same_grade\").isNotNull()) \\\n",
    "    .withColumn(\"days_between_loans\", fn.datediff(preprocessed_issue_date, preprocessed_prev_issue_date)) \\\n",
    "    .groupBy(\"grade\") \\\n",
    "    .agg(fn.avg(\"days_between_loans\").alias(\"avg_days_between_loans\"))\n",
    "\n",
    "q4_spark_df.show(q4_spark_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are exactly equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_dfs(q4_sql_df, q4_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Identify the average difference in loan amounts between consecutive loans\n",
    "within the same state and grade combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------------+\n",
      "|state|grade|avg_loan_amount_difference|\n",
      "+-----+-----+--------------------------+\n",
      "|    0|    1|                    2000.0|\n",
      "|    0|    2|                    8750.0|\n",
      "|    0|    3|        13333.333333333334|\n",
      "|    0|    4|                    8000.0|\n",
      "|    0|    5|                    8400.0|\n",
      "|    0|    6|                   11000.0|\n",
      "|    0|    7|        3333.3333333333335|\n",
      "|    0|    8|                    2200.0|\n",
      "|    0|    9|                    1600.0|\n",
      "|    0|   10|                   10000.0|\n",
      "|    0|   11|         3889.285714285714|\n",
      "|    0|   12|                   18000.0|\n",
      "|    0|   13|                    5500.0|\n",
      "|    0|   14|                    1500.0|\n",
      "|    0|   15|         1142.857142857143|\n",
      "|    0|   16|        11666.666666666666|\n",
      "|    0|   17|                    3500.0|\n",
      "|    0|   19|                   10637.5|\n",
      "|    0|   20|                   19225.0|\n",
      "|    0|   21|                   11200.0|\n",
      "|    0|   22|                   23000.0|\n",
      "|    0|   24|                   16000.0|\n",
      "|    0|   25|                    6400.0|\n",
      "|    0|   29|                   15000.0|\n",
      "|    1|    1|         3181.818181818182|\n",
      "|    1|    2|                     967.5|\n",
      "|    1|    3|        1818.1818181818182|\n",
      "|    1|    4|                    1500.0|\n",
      "|    1|    5|         789.4736842105264|\n",
      "|    1|    6|        1818.1818181818182|\n",
      "|    1|    7|        323.52941176470586|\n",
      "|    1|    8|         923.0769230769231|\n",
      "|    1|    9|                   1431.25|\n",
      "|    1|   10|         1923.076923076923|\n",
      "|    1|   11|        1014.7727272727273|\n",
      "|    1|   12|                     500.0|\n",
      "|    1|   13|         333.3333333333333|\n",
      "|    1|   14|        176.47058823529412|\n",
      "|    1|   15|        454.54545454545456|\n",
      "|    1|   16|                    2855.0|\n",
      "|    1|   17|        1772.9166666666667|\n",
      "|    1|   18|                    2000.0|\n",
      "|    1|   19|        2272.7272727272725|\n",
      "|    1|   20|                   2393.75|\n",
      "|    1|   21|                    2500.0|\n",
      "|    1|   22|                    2160.0|\n",
      "|    1|   23|                    5000.0|\n",
      "|    1|   24|                    7000.0|\n",
      "|    1|   25|         6383.333333333333|\n",
      "|    1|   27|                    4975.0|\n",
      "+-----+-----+--------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_5 = \"\"\"\n",
    "SELECT\n",
    "    state,\n",
    "    grade,\n",
    "    AVG(loan_amount - COALESCE(previous_loan_amount_same_state_and_grade, 0)) AS avg_loan_amount_difference\n",
    "FROM fintech_data\n",
    "GROUP BY state, grade\n",
    "\"\"\"\n",
    "# WHERE previous_loan_amount_same_state_and_grade IS NOT NULL\n",
    "\n",
    "q5_sql_df = spark.sql(query_5)\n",
    "q5_sql_df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------------+\n",
      "|state|grade|avg_loan_amount_difference|\n",
      "+-----+-----+--------------------------+\n",
      "|    0|    1|                    2000.0|\n",
      "|    0|    2|                    8750.0|\n",
      "|    0|    3|        13333.333333333334|\n",
      "|    0|    4|                    8000.0|\n",
      "|    0|    5|                    8400.0|\n",
      "|    0|    6|                   11000.0|\n",
      "|    0|    7|        3333.3333333333335|\n",
      "|    0|    8|                    2200.0|\n",
      "|    0|    9|                    1600.0|\n",
      "|    0|   10|                   10000.0|\n",
      "|    0|   11|         3889.285714285714|\n",
      "|    0|   12|                   18000.0|\n",
      "|    0|   13|                    5500.0|\n",
      "|    0|   14|                    1500.0|\n",
      "|    0|   15|         1142.857142857143|\n",
      "|    0|   16|        11666.666666666666|\n",
      "|    0|   17|                    3500.0|\n",
      "|    0|   19|                   10637.5|\n",
      "|    0|   20|                   19225.0|\n",
      "|    0|   21|                   11200.0|\n",
      "|    0|   22|                   23000.0|\n",
      "|    0|   24|                   16000.0|\n",
      "|    0|   25|                    6400.0|\n",
      "|    0|   29|                   15000.0|\n",
      "|    1|    1|         3181.818181818182|\n",
      "|    1|    2|                     967.5|\n",
      "|    1|    3|        1818.1818181818182|\n",
      "|    1|    4|                    1500.0|\n",
      "|    1|    5|         789.4736842105264|\n",
      "|    1|    6|        1818.1818181818182|\n",
      "|    1|    7|        323.52941176470586|\n",
      "|    1|    8|         923.0769230769231|\n",
      "|    1|    9|                   1431.25|\n",
      "|    1|   10|         1923.076923076923|\n",
      "|    1|   11|        1014.7727272727273|\n",
      "|    1|   12|                     500.0|\n",
      "|    1|   13|         333.3333333333333|\n",
      "|    1|   14|        176.47058823529412|\n",
      "|    1|   15|        454.54545454545456|\n",
      "|    1|   16|                    2855.0|\n",
      "|    1|   17|        1772.9166666666667|\n",
      "|    1|   18|                    2000.0|\n",
      "|    1|   19|        2272.7272727272725|\n",
      "|    1|   20|                   2393.75|\n",
      "|    1|   21|                    2500.0|\n",
      "|    1|   22|                    2160.0|\n",
      "|    1|   23|                    5000.0|\n",
      "|    1|   24|                    7000.0|\n",
      "|    1|   25|         6383.333333333333|\n",
      "|    1|   27|                    4975.0|\n",
      "+-----+-----+--------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loan_prev_diff = fn.col('loan_amount') - fn.coalesce(fn.col('previous_loan_amount_same_state_and_grade'), fn.lit(0))\n",
    "# .filter(fn.col('previous_loan_amount_same_state_and_grade').isNotNull())\n",
    "q5_spark_df = lagged_fintech_df_4 \\\n",
    "    .groupBy('state', 'grade') \\\n",
    "    .agg(fn.avg(loan_prev_diff).alias('avg_loan_amount_difference'))\n",
    "\n",
    "q5_spark_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are exactly equal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_equality_of_dfs(q5_sql_df, q5_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Lookup Table & Saving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load (save) the cleaned PySpark df and the lookup table to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_parquet(df: pyspark.sql.dataframe.DataFrame, path: str, should_repartition: bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Save a PySpark DataFrame to disk in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    path (str): Output path for the Parquet file.\n",
    "    should_repartition (bool): Whether to repartition the DataFrame before saving.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if should_repartition:\n",
    "        df = df.coalesce(1)\n",
    "    df.write.mode(\"overwrite\").parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_df_as_parquet(lagged_fintech_df_4, \"./cleaned_data/fintech_spark_52_1008_clean.parquet\")\n",
    "# save_df_as_parquet(GLOBAL_LOOKUP_TABLE, \"./cleaned_data/lookup_spark_52_1008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ./cleaned_data/fintech_spark_52_1008_clean.parquet\n",
      "DataFrame saved to ./cleaned_data/lookup_spark_52_1008.parquet\n"
     ]
    }
   ],
   "source": [
    "save_df_as_parquet(lagged_fintech_df_4, \"./cleaned_data/fintech_spark_52_1008_clean.parquet\", should_repartition=True)\n",
    "save_df_as_parquet(GLOBAL_LOOKUP_TABLE, \"./cleaned_data/lookup_spark_52_1008.parquet\", should_repartition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_parquet(path: str) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a PySpark DataFrame from disk in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "    path (str): Path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Loaded PySpark DataFrame.\n",
    "    \"\"\"\n",
    "    return spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_fintech_df = load_df_from_parquet(\"./cleaned_data/fintech_spark_52_1008_clean.parquet\")\n",
    "loaded_lookup_table = load_df_from_parquet(\"./cleaned_data/lookup_spark_52_1008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "|         customer_id|           emp_title|emp_length|annual_inc|annual_inc_joint|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|      issue_date|pymnt_plan|purpose|         description|home_ownership_own|home_ownership_any|home_ownership_mortgage|home_ownership_rent|verification_status_not verified|verification_status_verified|verification_status_source verified|type_joint|type_direct_pay|type_joint app|type_individual|letter_grade|previous_loan_issue_date_same_grade|previous_loan_amount_same_grade|previous_loan_date_same_state_and_grade|previous_loan_amount_same_state_and_grade|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "|YidceDEyJlx4Y2E+X...|Production Operator |       9.0|  110000.0|             0.0|   995xx|        AK|    41503.0|   456538.0| 109768|    Current|    11000.0|    0|      11000.0| 36 months|  0.0944|    6|    17 July 2017|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           B|                       17 July 2017|                        11000.0|                                   NULL|                                     NULL|\n",
      "|Yic7XHg5Y1x4ZDhZX...|    staff accountant|       1.0|   54000.0|             0.0|   995xx|        AK|     5507.0|    27537.0| 120238|    Current|    12000.0|    0|      12000.0| 36 months|  0.1099|    7|    16 July 2016|     false|      1|Credit card refin...|                 0|                 0|                      0|                  1|                               0|                           0|                                  1|         0|              0|             0|              1|           B|                       16 July 2016|                        12000.0|                                   NULL|                                     NULL|\n",
      "|YiduXHhjZlx4ODdce...|             Teacher|      11.0|   70000.0|             0.0|   995xx|        AK|     3895.0|    38945.0| 120113|    Current|    12000.0|    0|      12000.0| 36 months|  0.1099|    7|  16 August 2016|     false|      6|    Medical expenses|                 1|                 0|                      0|                  0|                               0|                           0|                                  1|         0|              0|             0|              1|           B|                     16 August 2016|                         5000.0|                           16 July 2016|                                  12000.0|\n",
      "|YidceDFjXHg5Nlx4Z...|             Teacher|       6.0|   70000.0|             0.0|   995xx|        AK|    26258.0|   262583.0|  91232|    Current|    10000.0|    0|      10000.0| 36 months|  0.1131|    7|19 February 2019|     false|      1|Credit card refin...|                 0|                 0|                      1|                  0|                               1|                           0|                                  0|         0|              0|             0|              1|           B|                   19 February 2019|                        12650.0|                         16 August 2016|                                  12000.0|\n",
      "|YidceGNmbVx4YWQgX...|   Territory Manager|       2.0|  108000.0|             0.0|   995xx|        AK|     1236.0|    14834.0| 243042|    Current|    30000.0|    0|      30000.0| 36 months|  0.1899|   19|   17 April 2017|     false|      2|  Debt consolidation|                 0|                 0|                      1|                  0|                               0|                           0|                                  1|         0|              0|             0|              1|           D|                      17 April 2017|                        28000.0|                                   NULL|                                     NULL|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+------------------+-----------------------+-------------------+--------------------------------+----------------------------+-----------------------------------+----------+---------------+--------------+---------------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_fintech_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+\n",
      "|    original_column|original_value|      encoded_column|\n",
      "+-------------------+--------------+--------------------+\n",
      "|     home_ownership|           own|  home_ownership_own|\n",
      "|     home_ownership|           any|  home_ownership_any|\n",
      "|     home_ownership|      mortgage|home_ownership_mo...|\n",
      "|     home_ownership|          rent| home_ownership_rent|\n",
      "|verification_status|  not verified|verification_stat...|\n",
      "+-------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_lookup_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Loading to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the cleaned parquet file and lookup table into a Postgres database.\n",
    "- Take Screenshots showing the newly added features in the feature engineering section\n",
    "- Take a screenshot from the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def save_to_db(df_path: str, table_name: str):\n",
    "    spark_df = load_df_from_parquet(df_path)\n",
    "    pd_df = spark_df.toPandas()\n",
    "    engine = create_engine('postgresql://root:root@pgdatabase:5432/testdb')\n",
    "    if(engine.connect()):\n",
    "        print('Connected to Database')\n",
    "        try:\n",
    "            print('Writing cleaned dataset to database')\n",
    "            pd_df.to_sql(table_name, con=engine, if_exists='replace')\n",
    "            print('Done writing to database')\n",
    "        except ValueError as vx:\n",
    "            print('Cleaned Table already exists.')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "    else:\n",
    "        print('Failed to connect to Database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Database\n",
      "Writing cleaned dataset to database\n",
      "Done writing to database\n"
     ]
    }
   ],
   "source": [
    "save_to_db(df_path='./cleaned_data/fintech_spark_52_1008_clean.parquet', table_name=\"fintech_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Database\n",
      "Writing cleaned dataset to database\n",
      "Done writing to database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35556)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "save_to_db(df_path='./cleaned_data/lookup_spark_52_1008.parquet', table_name=\"lookup_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
