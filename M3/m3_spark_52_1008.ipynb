{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window, Row, Column, functions as fn\n",
    "import psutil\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load the dataset from the parquet format given in the google drive above\n",
    "- Load the dataset.\n",
    "- Preview first 20 rows.\n",
    "- How many partitions is this dataframe split into?\n",
    "- Change partitions to be equal to the number of your logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"M3\").getOrCreate()\n",
    "#.config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.3.jar\").master(\"local\")\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", 'true')\n",
    "# spark.config(\"spark.memory.offHeap.enabled\",\"true\") \n",
    "# spark.config(\"spark.memory.offHeap.size\",\"10g\")\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# spark context to interact with the driver\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './work/fintech_data_29_52_1008.parquet'\n",
    "fintech_df = spark.read.parquet(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         Customer Id|           Emp Title|Emp Length|Home Ownership|Annual Inc|Annual Inc Joint|Verification Status|Zip Code|Addr State|Avg Cur Bal|Tot Cur Bal|Loan Id|Loan Status|Loan Amount|State|Funded Amount|      Term|Int Rate|Grade|       Issue Date|Pymnt Plan|      Type|           Purpose|         Description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidceDkzalx4YmRce...|     president/owner| 10+ years|      MORTGAGE|   80000.0|            NULL|    Source Verified|   333xx|        FL|     8275.0|   239986.0|  39474|    Current|     6000.0|   FL|       6000.0| 36 months|  0.0649|    4|   14 August 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yic0SVx4ZjRceGZlX...|PETTY OFFICER FIR...| 10+ years|      MORTGAGE|   75384.0|            NULL|    Source Verified|   237xx|        VA|    25385.0|   279232.0| 158200|    Current|    15000.0|   VA|      15000.0| 60 months|  0.1806|   20|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGUyXHgxZVx4M...|               nyco |   4 years|           OWN|   33800.0|            NULL|           Verified|   111xx|        NY|      282.0|     1691.0| 113752| Fully Paid|    11500.0|   NY|      11500.0| 36 months|  0.1114|    7| 12 December 2012|     false|INDIVIDUAL|             other|         Family Help|\n",
      "|YidcXD1ceDg2XHhhZ...|Nippon Express US...| 10+ years|           OWN|   50000.0|            NULL|           Verified|   070xx|        NJ|    14458.0|   130124.0| 181412| Fully Paid|    18000.0|   NJ|      18000.0| 60 months|  0.2149|   23| 12 November 2012|     false|INDIVIDUAL|       credit_card|  Credit Card Payoff|\n",
      "|YidceDgyXHgwNiZce...|  Operations Manager|   2 years|      MORTGAGE|   75000.0|        160000.0|       Not Verified|   750xx|        TX|    27703.0|   443252.0| 227090|    Current|    25000.0|   TX|      25000.0| 60 months|  0.1171|   10|19 September 2019|     false| Joint App|  home_improvement|    Home improvement|\n",
      "|YidceGU4XHhmYVdIX...|    flight attendant| 10+ years|      MORTGAGE|  110000.0|            NULL|           Verified|   762xx|        TX|     4799.0|   196782.0| 220157| Fully Paid|    24000.0|   TX|      24000.0| 60 months|  0.1899|   19|  17 January 2017|     false|Individual|  home_improvement|    Home improvement|\n",
      "|YicmXHg4Mlx4ODY/X...|       SALES MANAGER| 10+ years|          RENT|  100000.0|            NULL|    Source Verified|   452xx|        OH|     3627.0|    29019.0| 149100| Fully Paid|    15000.0|   OH|      15000.0| 36 months|  0.0967|    8| 13 November 2013|     false|INDIVIDUAL|debt_consolidation|  A BIRD IN THE HAND|\n",
      "|YiJceDgwXHg5ZSdGN...|maintenance mechanic|   4 years|           OWN|   70000.0|            NULL|           Verified|   125xx|        NY|     2599.0|    18193.0| 152910|    Current|    15000.0|   NY|      15000.0| 36 months|  0.1499|   14| 16 December 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YidceDBjXHhkZlx4Z...|       Administrator| 10+ years|      MORTGAGE|  117400.0|            NULL|    Source Verified|   082xx|        NJ|     9975.0|   259343.0| 214955| Fully Paid|    24000.0|   NJ|      24000.0| 36 months|  0.0692|    5|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidqXHhiMlx4ZDZaX...|   Senior Consultant|   2 years|          RENT|   85000.0|            NULL|       Not Verified|   105xx|        NY|     4182.0|    66907.0|   7032| Fully Paid|     2500.0|   NY|       2500.0| 36 months|  0.0799|    4|    17 March 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGE2XHg5NVx4Z...|             Manager|   6 years|          RENT|  110000.0|            NULL|       Not Verified|   342xx|        FL|     6631.0|    72939.0| 184546|    Current|    19175.0|   FL|      19175.0| 36 months|   0.288|   19|    19 April 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yic9K1x4MDhceGU2X...|Senior Structural...|    1 year|      MORTGAGE|   70000.0|            NULL|       Not Verified|   786xx|        TX|    24167.0|   265836.0| 167217|    Current|    16000.0|   TX|      16000.0| 60 months|  0.1288|   14| 15 December 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDk5XHhlMFx4M...|      Police Officer| 10+ years|          RENT|   70000.0|            NULL|       Not Verified|   337xx|        FL|     4993.0|    44934.0|   4621| Fully Paid|     2000.0|   FL|       2000.0| 36 months|  0.1299|   11|  15 January 2015|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidceDA1XHg4MGomX...|     Project Manager| 10+ years|          RENT|   55000.0|            NULL|    Source Verified|   372xx|        TN|     8173.0|    81725.0|  19826|    Current|     4000.0|   TN|       4000.0| 36 months|  0.1747|   18|     18 June 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidDXHg4Mlx4YWZce...|    Sales associate |   2 years|      MORTGAGE|   23000.0|            NULL|    Source Verified|   357xx|        AL|     7833.0|    23500.0|  28527|    Current|     5000.0|   AL|       5000.0| 36 months|  0.1042|    6|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidibFx4OTRceGI4X...|            Director|   5 years|      MORTGAGE|  120000.0|            NULL|       Not Verified|   925xx|        CA|    30724.0|   368683.0|  51952| Fully Paid|     7000.0|   CA|       7000.0| 36 months|  0.0624|    1| 15 November 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJXUjpceDg2XHg5O...|             Manager| 10+ years|      MORTGAGE|   82000.0|            NULL|       Not Verified|   357xx|        AL|    28152.0|   281524.0|  50695|    Current|     6600.0|   AL|       6600.0| 36 months|  0.1335|   12| 14 November 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGZmMj58XHhkY...|Network Administr...|   4 years|      MORTGAGE|   79000.0|            NULL|    Source Verified|   989xx|        WA|    12693.0|   165015.0| 256148| Fully Paid|    35000.0|   WA|      35000.0| 36 months|  0.1367|   12| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceDg0QVx4YThce...|       Staff Account|    1 year|      MORTGAGE|   35000.0|            NULL|       Not Verified|   331xx|        FL|     9902.0|   108921.0|  41140|    Current|     6000.0|   FL|       6000.0| 36 months|  0.0867|   10| 14 November 2014|     false|INDIVIDUAL|  home_improvement|    Home improvement|\n",
      "|Yic4Q1hceGI2XHg4Y...|           Inspector|   7 years|          RENT|   70000.0|            NULL|       Not Verified|   770xx|        TX|     3255.0|    32550.0| 152067| Fully Paid|    15000.0|   TX|      15000.0| 36 months|  0.1333|   15| 15 February 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(fintech_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical cores: 24\n",
      "Physical cores: 12\n"
     ]
    }
   ],
   "source": [
    "logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "physical_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "\n",
    "print(f\"Logical cores: {logical_cores}\")\n",
    "print(f\"Physical cores: {physical_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repartitioned_fintech_df = fintech_df.repartition(logical_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitioned_fintech_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rename all columns (replacing a space with an underscore, and making it lowercase)\n",
    "- Detect missing    \n",
    "  -  Create a function that takes in the df and returns any data structrue of your choice(df/dict,list,tuple,etc) which has the name of the column and percentage of missing entries from the whole dataset.\n",
    "  - Tip : storing the missing info as dict where the key is the column name and value is the percentage would be the easiest.\n",
    "  - Prinout the missing info\n",
    "- Handle missing\n",
    "  - For numerical features replace with 0.\n",
    "  - For categorical/strings replace with mode\n",
    "- Check missing\n",
    "  - Afterwards, check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer Id',\n",
       " 'Emp Title',\n",
       " 'Emp Length',\n",
       " 'Home Ownership',\n",
       " 'Annual Inc',\n",
       " 'Annual Inc Joint',\n",
       " 'Verification Status',\n",
       " 'Zip Code',\n",
       " 'Addr State',\n",
       " 'Avg Cur Bal',\n",
       " 'Tot Cur Bal',\n",
       " 'Loan Id',\n",
       " 'Loan Status',\n",
       " 'Loan Amount',\n",
       " 'State',\n",
       " 'Funded Amount',\n",
       " 'Term',\n",
       " 'Int Rate',\n",
       " 'Grade',\n",
       " 'Issue Date',\n",
       " 'Pymnt Plan',\n",
       " 'Type',\n",
       " 'Purpose',\n",
       " 'Description']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartitioned_fintech_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_fintech_df = repartitioned_fintech_df.toDF(*[col.replace(\" \", \"_\").lower() for col in repartitioned_fintech_df.columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'annual_inc_joint',\n",
       " 'verification_status',\n",
       " 'zip_code',\n",
       " 'addr_state',\n",
       " 'avg_cur_bal',\n",
       " 'tot_cur_bal',\n",
       " 'loan_id',\n",
       " 'loan_status',\n",
       " 'loan_amount',\n",
       " 'state',\n",
       " 'funded_amount',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'grade',\n",
       " 'issue_date',\n",
       " 'pymnt_plan',\n",
       " 'type',\n",
       " 'purpose',\n",
       " 'description']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_fintech_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         customer_id|           emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|    loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|       issue_date|pymnt_plan|      type|           purpose|         description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YicgXHgxOVx4ODU+X...| bartender/ waitress|   3 years|          RENT|   35000.0|            NULL|    Source Verified|   191xx|        PA|     2368.0|    16578.0| 139672|     Fully Paid|    14000.0|   PA|      14000.0| 36 months|  0.1999|   19|    17 March 2017|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidJXHhiOFx4OWRce...|      Line- Operator|  < 1 year|           OWN|   65000.0|            NULL|    Source Verified|   458xx|        OH|     1263.0|     3789.0| 207602|In Grace Period|    21275.0|   OH|      21275.0| 60 months|   0.124|    6|     19 July 2019|     false|Individual|    major_purchase|      Major purchase|\n",
      "|YidceGQwXHhmNl1ce...|                NULL|  < 1 year|      MORTGAGE|  100000.0|            NULL|           Verified|   152xx|        PA|    26234.0|   341037.0| 230358|     Fully Paid|    25350.0|   PA|      25350.0| 36 months|  0.1727|   19|    16 April 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yid6XHhkMlx4YmR8e...|   Corporate Trainer|   5 years|      MORTGAGE|   41000.0|            NULL|    Source Verified|   322xx|        FL|     3152.0|    18912.0|  98856|        Current|    10000.0|   FL|      10000.0| 36 months|     0.2|   16| 18 February 2018|     false|Individual|  home_improvement|    Home improvement|\n",
      "|Yid8XHhkZlpvdFJce...|Certified Applicator|   7 years|          RENT|   30000.0|            NULL|           Verified|   788xx|        TX|     2451.0|    19607.0|  33846|     Fully Paid|     5000.0|   TX|       5000.0| 36 months|  0.1899|   17| 17 February 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDk5XHhmY1x4M...|Associate Researcher|   4 years|           OWN|   67000.0|            NULL|       Not Verified|   598xx|        MT|     6283.0|    31415.0|  21295|     Fully Paid|     4250.0|   MT|       4250.0| 36 months|  0.1561|   20|    15 April 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDBiQlx4OWVce...|               sales| 10+ years|          RENT|   60000.0|            NULL|    Source Verified|   330xx|        FL|     1487.0|    19336.0| 167841|    Charged Off|    16000.0|   FL|      16000.0| 60 months|  0.1431|   14|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yid1Nlx4OTRceGQ1X...|Utility Plant  En...| 10+ years|      MORTGAGE|   82000.0|            NULL|       Not Verified|   088xx|        NJ|    30457.0|   335023.0| 235327|     Fully Paid|    28000.0|   NJ|      28000.0| 36 months|  0.0789|    2|   15 August 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yic5XHgxM1x4ZGRce...|      Office Manager|  < 1 year|      MORTGAGE|  125000.0|            NULL|    Source Verified|   346xx|        FL|     8382.0|   125734.0| 193947|        Current|    20000.0|   FL|      20000.0| 36 months|  0.1349|   15|16 September 2016|     false|INDIVIDUAL|  home_improvement|    Home improvement|\n",
      "|YidceGNiPFZ3XHhkN...|       Sales Manager|   2 years|          RENT|   40000.0|            NULL|       Not Verified|   606xx|        IL|     2656.0|    10624.0|  95200|     Fully Paid|    10000.0|   IL|      10000.0| 36 months|  0.1398|   12| 13 December 2013|     false|INDIVIDUAL|debt_consolidation|Good Bet Debt Con...|\n",
      "|YidceGZkOFx4ZTZce...|Logistics coordin...|   3 years|          RENT|   47800.0|            NULL|           Verified|   902xx|        CA|     2855.0|    14276.0|  40657|        Current|     6000.0|   CA|       6000.0| 36 months|  0.0797|    3|      17 May 2017|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDE3TzQuXHhhY...|           Echo tech|  < 1 year|          RENT|   64999.0|            NULL|       Not Verified|   481xx|        MI|     4442.0|   133252.0|  20325|        Current|     4050.0|   MI|       4050.0| 36 months|  0.1102|   10| 19 November 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidLXHhjMFx4ZDVce...|      LOAN PROCESSOR| 10+ years|      MORTGAGE|   75000.0|            NULL|           Verified|   365xx|        AL|    12223.0|   293351.0| 158526|        Current|    15000.0|   AL|      15000.0| 60 months|  0.1953|   20|    16 April 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YicoXHg5MFx4ZjFce...|       HR consultant|   8 years|      MORTGAGE|   75000.0|            NULL|       Not Verified|   840xx|        UT|    16220.0|   243302.0| 186598|        Current|    19825.0|   UT|      19825.0| 60 months|  0.2399|   29|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidDXHhjMDVceDFlX...|Director of Stude...|    1 year|          RENT|   71937.0|            NULL|    Source Verified|   852xx|        AZ|     9603.0|   105631.0| 231429|        Current|    26000.0|   AZ|      26000.0| 36 months|  0.1499|   13|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGJjXHhiNVx4Y...|    Building Manager| 10+ years|          RENT|   51000.0|            NULL|       Not Verified|   801xx|        CO|     3237.0|    16184.0|   1664|        Current|     1200.0|   CO|       1200.0| 36 months|  0.1942|   16|     18 June 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFkXHg4ZVx4O...|Human Resource Ma...|   9 years|      MORTGAGE|   58000.0|            NULL|       Not Verified|   146xx|        NY|     7212.0|   122608.0|  70546|        Current|     8400.0|   NY|       8400.0| 36 months|  0.1049|    7| 16 November 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yiczflx4YjUqXHhlN...|             Teacher|  < 1 year|      MORTGAGE|  120000.0|            NULL|           Verified|   891xx|        NV|    20113.0|   201125.0| 253983|     Fully Paid|    35000.0|   NV|      35000.0| 36 months|  0.0724|    1|  17 January 2017|     false|Individual|  home_improvement|    Home improvement|\n",
      "|YiJyJ1x4ODZceDAwX...| Deportation officer| 10+ years|      MORTGAGE|  115000.0|            NULL|           Verified|   919xx|        CA|    66538.0|   598839.0| 243836|     Fully Paid|    30000.0|   CA|      30000.0| 60 months|  0.1008|    6|18 September 2018|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicrXHgxOFx4Y2Nce...|Internet Sales Co...|  < 1 year|      MORTGAGE|   37000.0|            NULL|       Not Verified|   805xx|        CO|    22918.0|   206261.0|  86629|     Fully Paid|    10000.0|   CO|      10000.0| 36 months|  0.0818|    7|  15 October 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_fintech_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missing(df: pyspark.sql.dataframe.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detect missing values in a PySpark DataFrame and calculate the percentage of missing entries.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are column names and values are percentages of missing values.\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    missing_info = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(fn.col(column).isNull()).count()\n",
    "        missing_percentage = (missing_count / total_rows) * 100\n",
    "        missing_info[column] = missing_percentage\n",
    "    \n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dict = detect_missing(renamed_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 8.612652608213097,\n",
       " 'emp_length': 6.777654458009619,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 93.11875693673696,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 4.384017758046615,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.8139104698483166}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- loan_id: long (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- funded_amount: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- grade: long (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- pymnt_plan: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamed_fintech_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annual_inc',\n",
       " 'annual_inc_joint',\n",
       " 'avg_cur_bal',\n",
       " 'tot_cur_bal',\n",
       " 'loan_id',\n",
       " 'loan_amount',\n",
       " 'funded_amount',\n",
       " 'int_rate',\n",
       " 'grade']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_types = (pyspark.sql.types.DoubleType, pyspark.sql.types.FloatType, pyspark.sql.types.IntegerType, pyspark.sql.types.LongType)\n",
    "numerical_columns = [field.name for field in renamed_fintech_df.schema.fields if isinstance(field.dataType, numerical_types)]\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def handle_missing_numerical(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace missing values in numerical columns with 0.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with missing values in numerical columns replaced by 0.\n",
    "    \"\"\"\n",
    "    missing_dict = detect_missing(df)\n",
    "    numerical_types = (pyspark.sql.types.DoubleType, pyspark.sql.types.FloatType, pyspark.sql.types.IntegerType, pyspark.sql.types.LongType)\n",
    "    numerical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, numerical_types)]\n",
    "    for column in numerical_columns:\n",
    "        if missing_dict[column] > 0:\n",
    "            df = df.fillna({column: 0})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerically_imputed_df = handle_missing_numerical(renamed_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 8.612652608213097,\n",
       " 'emp_length': 6.777654458009619,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 0.0,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 0.0,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.8139104698483166}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict_updated = detect_missing(numerically_imputed_df)\n",
    "missing_dict_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_categorical(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace missing values in categorical/string columns with the mode (most frequent value).\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with missing values in categorical columns replaced by mode.\n",
    "    \"\"\"\n",
    "    missing_dict = detect_missing(df)\n",
    "    categorical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, pyspark.sql.types.StringType)]\n",
    "    for column in categorical_columns:\n",
    "        if missing_dict[column] > 0:\n",
    "            mode_col = df.filter(fn.col(column).isNotNull()).groupBy(column).count().orderBy(fn.col('count').desc()).limit(1)\n",
    "            mode_col.show()\n",
    "            mode_value = mode_col.select(column).collect()[0][0]\n",
    "            print(f\"Mode value for {column}: {mode_value}\")\n",
    "            df = df.fillna(value= mode_value, subset=[column])\n",
    "            print('-'* 50)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|emp_title|count|\n",
      "+---------+-----+\n",
      "|  Teacher|  468|\n",
      "+---------+-----+\n",
      "\n",
      "Mode value for emp_title: Teacher\n",
      "--------------------------------------------------\n",
      "+----------+-----+\n",
      "|emp_length|count|\n",
      "+----------+-----+\n",
      "| 10+ years| 8855|\n",
      "+----------+-----+\n",
      "\n",
      "Mode value for emp_length: 10+ years\n",
      "--------------------------------------------------\n",
      "+------------------+-----+\n",
      "|       description|count|\n",
      "+------------------+-----+\n",
      "|Debt consolidation|14421|\n",
      "+------------------+-----+\n",
      "\n",
      "Mode value for description: Debt consolidation\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "categorically_imputed_df = handle_missing_categorical(numerically_imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0.0,\n",
       " 'emp_title': 0.0,\n",
       " 'emp_length': 0.0,\n",
       " 'home_ownership': 0.0,\n",
       " 'annual_inc': 0.0,\n",
       " 'annual_inc_joint': 0.0,\n",
       " 'verification_status': 0.0,\n",
       " 'zip_code': 0.0,\n",
       " 'addr_state': 0.0,\n",
       " 'avg_cur_bal': 0.0,\n",
       " 'tot_cur_bal': 0.0,\n",
       " 'loan_id': 0.0,\n",
       " 'loan_status': 0.0,\n",
       " 'loan_amount': 0.0,\n",
       " 'state': 0.0,\n",
       " 'funded_amount': 0.0,\n",
       " 'term': 0.0,\n",
       " 'int_rate': 0.0,\n",
       " 'grade': 0.0,\n",
       " 'issue_date': 0.0,\n",
       " 'pymnt_plan': 0.0,\n",
       " 'type': 0.0,\n",
       " 'purpose': 0.0,\n",
       " 'description': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dict_updated = detect_missing(categorically_imputed_df)\n",
    "missing_dict_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no missing values are remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode only the following categorical values\n",
    "- Emp Length: Change to numerical\n",
    "- Home Ownership: One Hot Encoding\n",
    "- Verification Status: One Hot Encoding\n",
    "- State: Label Encoding\n",
    "- Type: One Hot Encoding\n",
    "- Purpose: Label Encoding\n",
    "- For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "DO NOT Encode the employment title of description or any other column that is not\n",
    "mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LOOKUP_TABLE = spark.createDataFrame([], schema=\"original_column STRING, original_value STRING, encoded_column STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OWN'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorically_imputed_df.select('home_ownership').distinct().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|home_ownership|\n",
      "+--------------+\n",
      "|           OWN|\n",
      "|          RENT|\n",
      "|      MORTGAGE|\n",
      "|           ANY|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorically_imputed_df.select(\"home_ownership\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df: pyspark.sql.dataframe.DataFrame, columns: List[str]) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply one-hot encoding to the specified categorical columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    columns (list): List of column names to one-hot encode.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    lookup_data = []\n",
    "    for column in columns:\n",
    "        unique_values = df.select(column).distinct().collect()\n",
    "        \n",
    "        for row in unique_values:\n",
    "            curr_value = row[0]\n",
    "            encoded_col_name = f\"{column}_{curr_value}\"\n",
    "            df = df.withColumn(encoded_col_name, fn.when(fn.col(column) == curr_value, 1).otherwise(0))\n",
    "            df = df.withColumn(encoded_col_name, df[encoded_col_name].cast(\"int\"))\n",
    "            lookup_data.append(Row(original_column=column, original_value=curr_value, encoded_column=encoded_col_name))\n",
    "            \n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    print(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "    df = df.drop(*columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 28|\n",
      "|    Bob| 35|\n",
      "|Charlie| 40|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 28), (\"Bob\", 35), (\"Charlie\", 40)]\n",
    "\n",
    "# Create DataFrame\n",
    "ff = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "ff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(original_column='home_ownership', original_value='OWN', encoded_column='home_ownership_OWN'), Row(original_column='home_ownership', original_value='RENT', encoded_column='home_ownership_RENT'), Row(original_column='home_ownership', original_value='MORTGAGE', encoded_column='home_ownership_MORTGAGE'), Row(original_column='home_ownership', original_value='ANY', encoded_column='home_ownership_ANY'), Row(original_column='verification_status', original_value='Verified', encoded_column='verification_status_Verified'), Row(original_column='verification_status', original_value='Source Verified', encoded_column='verification_status_Source Verified'), Row(original_column='verification_status', original_value='Not Verified', encoded_column='verification_status_Not Verified'), Row(original_column='type', original_value='Joint App', encoded_column='type_Joint App'), Row(original_column='type', original_value='Individual', encoded_column='type_Individual'), Row(original_column='type', original_value='DIRECT_PAY', encoded_column='type_DIRECT_PAY'), Row(original_column='type', original_value='JOINT', encoded_column='type_JOINT'), Row(original_column='type', original_value='INDIVIDUAL', encoded_column='type_INDIVIDUAL')]\n"
     ]
    }
   ],
   "source": [
    "ohe_columns = [\"home_ownership\", \"verification_status\", \"type\"]\n",
    "ohe_fintech_df = one_hot_encode(categorically_imputed_df, ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['customer_id',\n",
       "  'emp_title',\n",
       "  'emp_length',\n",
       "  'annual_inc',\n",
       "  'annual_inc_joint',\n",
       "  'zip_code',\n",
       "  'addr_state',\n",
       "  'avg_cur_bal',\n",
       "  'tot_cur_bal',\n",
       "  'loan_id',\n",
       "  'loan_status',\n",
       "  'loan_amount',\n",
       "  'state',\n",
       "  'funded_amount',\n",
       "  'term',\n",
       "  'int_rate',\n",
       "  'grade',\n",
       "  'issue_date',\n",
       "  'pymnt_plan',\n",
       "  'purpose',\n",
       "  'description',\n",
       "  'home_ownership_OWN',\n",
       "  'home_ownership_RENT',\n",
       "  'home_ownership_MORTGAGE',\n",
       "  'home_ownership_ANY',\n",
       "  'verification_status_Verified',\n",
       "  'verification_status_Source Verified',\n",
       "  'verification_status_Not Verified',\n",
       "  'type_Joint App',\n",
       "  'type_INDIVIDUAL',\n",
       "  'type_DIRECT_PAY',\n",
       "  'type_JOINT'],\n",
       " 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_fintech_df.columns, len(ohe_fintech_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|home_ownership_MORTGAGE|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      0|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe_fintech_df.select(['home_ownership_MORTGAGE']).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state='AK'),\n",
       " Row(state='AL'),\n",
       " Row(state='AR'),\n",
       " Row(state='AZ'),\n",
       " Row(state='CA'),\n",
       " Row(state='CO'),\n",
       " Row(state='CT'),\n",
       " Row(state='DC'),\n",
       " Row(state='DE'),\n",
       " Row(state='FL'),\n",
       " Row(state='GA'),\n",
       " Row(state='HI'),\n",
       " Row(state='ID'),\n",
       " Row(state='IL'),\n",
       " Row(state='IN'),\n",
       " Row(state='KS'),\n",
       " Row(state='KY'),\n",
       " Row(state='LA'),\n",
       " Row(state='MA'),\n",
       " Row(state='MD'),\n",
       " Row(state='ME'),\n",
       " Row(state='MI'),\n",
       " Row(state='MN'),\n",
       " Row(state='MO'),\n",
       " Row(state='MS'),\n",
       " Row(state='MT'),\n",
       " Row(state='NC'),\n",
       " Row(state='ND'),\n",
       " Row(state='NE'),\n",
       " Row(state='NH'),\n",
       " Row(state='NJ'),\n",
       " Row(state='NM'),\n",
       " Row(state='NV'),\n",
       " Row(state='NY'),\n",
       " Row(state='OH'),\n",
       " Row(state='OK'),\n",
       " Row(state='OR'),\n",
       " Row(state='PA'),\n",
       " Row(state='RI'),\n",
       " Row(state='SC'),\n",
       " Row(state='SD'),\n",
       " Row(state='TN'),\n",
       " Row(state='TX'),\n",
       " Row(state='UT'),\n",
       " Row(state='VA'),\n",
       " Row(state='VT'),\n",
       " Row(state='WA'),\n",
       " Row(state='WI'),\n",
       " Row(state='WV'),\n",
       " Row(state='WY')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_fintech_df.select(\"state\").distinct().sort(\"state\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df: pyspark.sql.dataframe.DataFrame, columns: List[str]) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply label encoding to the specified categorical columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    columns (list): List of column names to label encode.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with label-encoded columns.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    lookup_data = []\n",
    "    for column in columns:\n",
    "        state_values = df.select(column).distinct().sort(column).collect()\n",
    "        for i, row in enumerate(state_values):\n",
    "            value = row[0]\n",
    "            df = df.withColumn(column, fn.when(df[column] == value, i).otherwise(df[column]))\n",
    "            lookup_data.append(Row(original_column=column, original_value=value, encoded_column=i))\n",
    "        df = df.withColumn(column, df[column].cast(\"int\"))\n",
    "    print(lookup_data)\n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(original_column='state', original_value='AK', encoded_column=0), Row(original_column='state', original_value='AL', encoded_column=1), Row(original_column='state', original_value='AR', encoded_column=2), Row(original_column='state', original_value='AZ', encoded_column=3), Row(original_column='state', original_value='CA', encoded_column=4), Row(original_column='state', original_value='CO', encoded_column=5), Row(original_column='state', original_value='CT', encoded_column=6), Row(original_column='state', original_value='DC', encoded_column=7), Row(original_column='state', original_value='DE', encoded_column=8), Row(original_column='state', original_value='FL', encoded_column=9), Row(original_column='state', original_value='GA', encoded_column=10), Row(original_column='state', original_value='HI', encoded_column=11), Row(original_column='state', original_value='ID', encoded_column=12), Row(original_column='state', original_value='IL', encoded_column=13), Row(original_column='state', original_value='IN', encoded_column=14), Row(original_column='state', original_value='KS', encoded_column=15), Row(original_column='state', original_value='KY', encoded_column=16), Row(original_column='state', original_value='LA', encoded_column=17), Row(original_column='state', original_value='MA', encoded_column=18), Row(original_column='state', original_value='MD', encoded_column=19), Row(original_column='state', original_value='ME', encoded_column=20), Row(original_column='state', original_value='MI', encoded_column=21), Row(original_column='state', original_value='MN', encoded_column=22), Row(original_column='state', original_value='MO', encoded_column=23), Row(original_column='state', original_value='MS', encoded_column=24), Row(original_column='state', original_value='MT', encoded_column=25), Row(original_column='state', original_value='NC', encoded_column=26), Row(original_column='state', original_value='ND', encoded_column=27), Row(original_column='state', original_value='NE', encoded_column=28), Row(original_column='state', original_value='NH', encoded_column=29), Row(original_column='state', original_value='NJ', encoded_column=30), Row(original_column='state', original_value='NM', encoded_column=31), Row(original_column='state', original_value='NV', encoded_column=32), Row(original_column='state', original_value='NY', encoded_column=33), Row(original_column='state', original_value='OH', encoded_column=34), Row(original_column='state', original_value='OK', encoded_column=35), Row(original_column='state', original_value='OR', encoded_column=36), Row(original_column='state', original_value='PA', encoded_column=37), Row(original_column='state', original_value='RI', encoded_column=38), Row(original_column='state', original_value='SC', encoded_column=39), Row(original_column='state', original_value='SD', encoded_column=40), Row(original_column='state', original_value='TN', encoded_column=41), Row(original_column='state', original_value='TX', encoded_column=42), Row(original_column='state', original_value='UT', encoded_column=43), Row(original_column='state', original_value='VA', encoded_column=44), Row(original_column='state', original_value='VT', encoded_column=45), Row(original_column='state', original_value='WA', encoded_column=46), Row(original_column='state', original_value='WI', encoded_column=47), Row(original_column='state', original_value='WV', encoded_column=48), Row(original_column='state', original_value='WY', encoded_column=49), Row(original_column='purpose', original_value='car', encoded_column=0), Row(original_column='purpose', original_value='credit_card', encoded_column=1), Row(original_column='purpose', original_value='debt_consolidation', encoded_column=2), Row(original_column='purpose', original_value='home_improvement', encoded_column=3), Row(original_column='purpose', original_value='house', encoded_column=4), Row(original_column='purpose', original_value='major_purchase', encoded_column=5), Row(original_column='purpose', original_value='medical', encoded_column=6), Row(original_column='purpose', original_value='moving', encoded_column=7), Row(original_column='purpose', original_value='other', encoded_column=8), Row(original_column='purpose', original_value='renewable_energy', encoded_column=9), Row(original_column='purpose', original_value='small_business', encoded_column=10), Row(original_column='purpose', original_value='vacation', encoded_column=11), Row(original_column='purpose', original_value='wedding', encoded_column=12)]\n"
     ]
    }
   ],
   "source": [
    "label_columns = [\"state\", \"purpose\"]\n",
    "label_fintech_df = label_encode(ohe_fintech_df, label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_fintech_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "|    7|\n",
      "|    8|\n",
      "|    9|\n",
      "|   10|\n",
      "|   11|\n",
      "|   12|\n",
      "|   13|\n",
      "|   14|\n",
      "|   15|\n",
      "|   16|\n",
      "|   17|\n",
      "|   18|\n",
      "|   19|\n",
      "|   20|\n",
      "|   21|\n",
      "|   22|\n",
      "|   23|\n",
      "|   24|\n",
      "|   25|\n",
      "|   26|\n",
      "|   27|\n",
      "|   28|\n",
      "|   29|\n",
      "|   30|\n",
      "|   31|\n",
      "|   32|\n",
      "|   33|\n",
      "|   34|\n",
      "|   35|\n",
      "|   36|\n",
      "|   37|\n",
      "|   38|\n",
      "|   39|\n",
      "|   40|\n",
      "|   41|\n",
      "|   42|\n",
      "|   43|\n",
      "|   44|\n",
      "|   45|\n",
      "|   46|\n",
      "|   47|\n",
      "|   48|\n",
      "|   49|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_fintech_df.select(\"state\").distinct().sort('state').show(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A (1-5)\n",
    "- B (6-10)\n",
    "- C (11-15)\n",
    "- D (16-20)\n",
    "- E (21-25)\n",
    "- F (26-30)\n",
    "- G (31-35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_letter_grade(df: pyspark.sql.dataframe.DataFrame, grade_column: str=\"grade\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column `letter_grade` based on the numerical range of `grade`.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    grade_column (str): Name of the column containing numerical grades.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with an additional `letter_grade` column.\n",
    "    \"\"\"\n",
    "    global GLOBAL_LOOKUP_TABLE\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "    lookup_data = []\n",
    "    for i, letter in enumerate(letters):\n",
    "        for j in range((i * 5)+1, ((i + 1) * 5)+1):\n",
    "            lookup_data.append(Row(original_column=grade_column, original_value=str(j), encoded_value=letter))\n",
    "    lookup_table = spark.createDataFrame(lookup_data)\n",
    "    print(lookup_data)\n",
    "    GLOBAL_LOOKUP_TABLE = GLOBAL_LOOKUP_TABLE.union(lookup_table)\n",
    "\n",
    "    \n",
    "    grade_mapping = [\n",
    "        (fn.col(grade_column).between(1, 5), \"A\"),\n",
    "        (fn.col(grade_column).between(6, 10), \"B\"),\n",
    "        (fn.col(grade_column).between(11, 15), \"C\"),\n",
    "        (fn.col(grade_column).between(16, 20), \"D\"),\n",
    "        (fn.col(grade_column).between(21, 25), \"E\"),\n",
    "        (fn.col(grade_column).between(26, 30), \"F\"),\n",
    "        (fn.col(grade_column).between(31, 35), \"G\"),\n",
    "    ]\n",
    "    \n",
    "    letter_grade_column = fn.when(*grade_mapping[0])\n",
    "    for condition, letter in grade_mapping[1:]:\n",
    "        letter_grade_column = letter_grade_column.when(condition, letter)\n",
    "    letter_grade_column = letter_grade_column.otherwise(\"Unknown\")\n",
    "\n",
    "    df = df.withColumn(\"letter_grade\", letter_grade_column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(original_column='grade', original_value='1', encoded_value='A'), Row(original_column='grade', original_value='2', encoded_value='A'), Row(original_column='grade', original_value='3', encoded_value='A'), Row(original_column='grade', original_value='4', encoded_value='A'), Row(original_column='grade', original_value='5', encoded_value='A'), Row(original_column='grade', original_value='6', encoded_value='B'), Row(original_column='grade', original_value='7', encoded_value='B'), Row(original_column='grade', original_value='8', encoded_value='B'), Row(original_column='grade', original_value='9', encoded_value='B'), Row(original_column='grade', original_value='10', encoded_value='B'), Row(original_column='grade', original_value='11', encoded_value='C'), Row(original_column='grade', original_value='12', encoded_value='C'), Row(original_column='grade', original_value='13', encoded_value='C'), Row(original_column='grade', original_value='14', encoded_value='C'), Row(original_column='grade', original_value='15', encoded_value='C'), Row(original_column='grade', original_value='16', encoded_value='D'), Row(original_column='grade', original_value='17', encoded_value='D'), Row(original_column='grade', original_value='18', encoded_value='D'), Row(original_column='grade', original_value='19', encoded_value='D'), Row(original_column='grade', original_value='20', encoded_value='D'), Row(original_column='grade', original_value='21', encoded_value='E'), Row(original_column='grade', original_value='22', encoded_value='E'), Row(original_column='grade', original_value='23', encoded_value='E'), Row(original_column='grade', original_value='24', encoded_value='E'), Row(original_column='grade', original_value='25', encoded_value='E'), Row(original_column='grade', original_value='26', encoded_value='F'), Row(original_column='grade', original_value='27', encoded_value='F'), Row(original_column='grade', original_value='28', encoded_value='F'), Row(original_column='grade', original_value='29', encoded_value='F'), Row(original_column='grade', original_value='30', encoded_value='F'), Row(original_column='grade', original_value='31', encoded_value='G'), Row(original_column='grade', original_value='32', encoded_value='G'), Row(original_column='grade', original_value='33', encoded_value='G'), Row(original_column='grade', original_value='34', encoded_value='G'), Row(original_column='grade', original_value='35', encoded_value='G')]\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df = create_letter_grade(label_fintech_df, \"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|grade|letter_grade|\n",
      "+-----+------------+\n",
      "|    1|           A|\n",
      "|    2|           A|\n",
      "|    3|           A|\n",
      "|    4|           A|\n",
      "|    5|           A|\n",
      "|    6|           B|\n",
      "|    7|           B|\n",
      "|    8|           B|\n",
      "|    9|           B|\n",
      "|   10|           B|\n",
      "|   11|           C|\n",
      "|   12|           C|\n",
      "|   13|           C|\n",
      "|   14|           C|\n",
      "|   15|           C|\n",
      "|   16|           D|\n",
      "|   17|           D|\n",
      "|   18|           D|\n",
      "|   19|           D|\n",
      "|   20|           D|\n",
      "|   21|           E|\n",
      "|   22|           E|\n",
      "|   23|           E|\n",
      "|   24|           E|\n",
      "|   25|           E|\n",
      "|   26|           F|\n",
      "|   27|           F|\n",
      "|   28|           F|\n",
      "|   29|           F|\n",
      "|   30|           F|\n",
      "|   31|           G|\n",
      "|   32|           G|\n",
      "|   33|           G|\n",
      "|   34|           G|\n",
      "|   35|           G|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df.select(\"grade\", \"letter_grade\").distinct().sort('grade').show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|emp_length|\n",
      "+----------+\n",
      "|   5 years|\n",
      "|   9 years|\n",
      "|    1 year|\n",
      "|   2 years|\n",
      "|   7 years|\n",
      "|   8 years|\n",
      "|   4 years|\n",
      "|   6 years|\n",
      "|   3 years|\n",
      "| 10+ years|\n",
      "|  < 1 year|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df.select(\"emp_length\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "def convert_emp_length_to_numeric(df: pyspark.sql.dataframe.DataFrame, column: str = \"emp_length\") -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'emp_length' column to numeric values using string manipulation.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    column (str): Column name containing employment length data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with 'emp_length' column converted to numeric values.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(column, fn.regexp_replace(fn.col(column), \"years|year\", \"\"))\n",
    "    df = df.withColumn(column, fn.when(fn.col(column).like(\"%<%\"), \"0.5\")\n",
    "                                 .when(fn.col(column).like(\"%+%\"), \"11\")\n",
    "                                 .otherwise(fn.col(column)))\n",
    "    df = df.withColumn(column, fn.col(column).cast(\"float\"))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_fintech_df_2 = convert_emp_length_to_numeric(encoded_fintech_df, \"emp_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|emp_length|\n",
      "+----------+\n",
      "|       0.5|\n",
      "|       1.0|\n",
      "|       2.0|\n",
      "|       3.0|\n",
      "|       4.0|\n",
      "|       5.0|\n",
      "|       6.0|\n",
      "|       7.0|\n",
      "|       8.0|\n",
      "|       9.0|\n",
      "|      11.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df_2.select(\"emp_length\").distinct().sort(\"emp_length\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+\n",
      "|         customer_id|           emp_title|emp_length|annual_inc|annual_inc_joint|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|    loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|       issue_date|pymnt_plan|purpose|         description|home_ownership_OWN|home_ownership_RENT|home_ownership_MORTGAGE|home_ownership_ANY|verification_status_Verified|verification_status_Source Verified|verification_status_Not Verified|type_Joint App|type_INDIVIDUAL|type_DIRECT_PAY|type_JOINT|letter_grade|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+\n",
      "|YicgXHgxOVx4ODU+X...| bartender/ waitress|       3.0|   35000.0|             0.0|   191xx|        PA|     2368.0|    16578.0| 139672|     Fully Paid|    14000.0|   37|      14000.0| 36 months|  0.1999|   19|    17 March 2017|     false|      5|      Major purchase|                 0|                  1|                      0|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           D|\n",
      "|YidJXHhiOFx4OWRce...|      Line- Operator|       0.5|   65000.0|             0.0|   458xx|        OH|     1263.0|     3789.0| 207602|In Grace Period|    21275.0|   34|      21275.0| 60 months|   0.124|    6|     19 July 2019|     false|      5|      Major purchase|                 1|                  0|                      0|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           B|\n",
      "|YidceGQwXHhmNl1ce...|             Teacher|       0.5|  100000.0|             0.0|   152xx|        PA|    26234.0|   341037.0| 230358|     Fully Paid|    25350.0|   37|      25350.0| 36 months|  0.1727|   19|    16 April 2016|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           1|                                  0|                               0|             0|              1|              0|         0|           D|\n",
      "|Yid6XHhkMlx4YmR8e...|   Corporate Trainer|       5.0|   41000.0|             0.0|   322xx|        FL|     3152.0|    18912.0|  98856|        Current|    10000.0|    9|      10000.0| 36 months|     0.2|   16| 18 February 2018|     false|      3|    Home improvement|                 0|                  0|                      1|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           D|\n",
      "|Yid8XHhkZlpvdFJce...|Certified Applicator|       7.0|   30000.0|             0.0|   788xx|        TX|     2451.0|    19607.0|  33846|     Fully Paid|     5000.0|   42|       5000.0| 36 months|  0.1899|   17| 17 February 2017|     false|      2|  Debt consolidation|                 0|                  1|                      0|                 0|                           1|                                  0|                               0|             0|              0|              0|         0|           D|\n",
      "|YidceDk5XHhmY1x4M...|Associate Researcher|       4.0|   67000.0|             0.0|   598xx|        MT|     6283.0|    31415.0|  21295|     Fully Paid|     4250.0|   25|       4250.0| 36 months|  0.1561|   20|    15 April 2015|     false|      2|  Debt consolidation|                 1|                  0|                      0|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           D|\n",
      "|YidceDBiQlx4OWVce...|               sales|      11.0|   60000.0|             0.0|   330xx|        FL|     1487.0|    19336.0| 167841|    Charged Off|    16000.0|    9|      16000.0| 60 months|  0.1431|   14|  15 January 2015|     false|      2|  Debt consolidation|                 0|                  1|                      0|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           C|\n",
      "|Yid1Nlx4OTRceGQ1X...|Utility Plant  En...|      11.0|   82000.0|             0.0|   088xx|        NJ|    30457.0|   335023.0| 235327|     Fully Paid|    28000.0|   30|      28000.0| 36 months|  0.0789|    2|   15 August 2015|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           A|\n",
      "|Yic5XHgxM1x4ZGRce...|      Office Manager|       0.5|  125000.0|             0.0|   346xx|        FL|     8382.0|   125734.0| 193947|        Current|    20000.0|    9|      20000.0| 36 months|  0.1349|   15|16 September 2016|     false|      3|    Home improvement|                 0|                  0|                      1|                 0|                           0|                                  1|                               0|             0|              1|              0|         0|           C|\n",
      "|YidceGNiPFZ3XHhkN...|       Sales Manager|       2.0|   40000.0|             0.0|   606xx|        IL|     2656.0|    10624.0|  95200|     Fully Paid|    10000.0|   13|      10000.0| 36 months|  0.1398|   12| 13 December 2013|     false|      2|Good Bet Debt Con...|                 0|                  1|                      0|                 0|                           0|                                  0|                               1|             0|              1|              0|         0|           C|\n",
      "|YidceGZkOFx4ZTZce...|Logistics coordin...|       3.0|   47800.0|             0.0|   902xx|        CA|     2855.0|    14276.0|  40657|        Current|     6000.0|    4|       6000.0| 36 months|  0.0797|    3|      17 May 2017|     false|      1|Credit card refin...|                 0|                  1|                      0|                 0|                           1|                                  0|                               0|             0|              0|              0|         0|           A|\n",
      "|YidceDE3TzQuXHhhY...|           Echo tech|       0.5|   64999.0|             0.0|   481xx|        MI|     4442.0|   133252.0|  20325|        Current|     4050.0|   21|       4050.0| 36 months|  0.1102|   10| 19 November 2019|     false|      1|Credit card refin...|                 0|                  1|                      0|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           B|\n",
      "|YidLXHhjMFx4ZDVce...|      LOAN PROCESSOR|      11.0|   75000.0|             0.0|   365xx|        AL|    12223.0|   293351.0| 158526|        Current|    15000.0|    1|      15000.0| 60 months|  0.1953|   20|    16 April 2016|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           1|                                  0|                               0|             0|              1|              0|         0|           D|\n",
      "|YicoXHg5MFx4ZjFce...|       HR consultant|       8.0|   75000.0|             0.0|   840xx|        UT|    16220.0|   243302.0| 186598|        Current|    19825.0|   43|      19825.0| 60 months|  0.2399|   29|  15 January 2015|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           F|\n",
      "|YidDXHhjMDVceDFlX...|Director of Stude...|       1.0|   71937.0|             0.0|   852xx|        AZ|     9603.0|   105631.0| 231429|        Current|    26000.0|    3|      26000.0| 36 months|  0.1499|   13|14 September 2014|     false|      2|  Debt consolidation|                 0|                  1|                      0|                 0|                           0|                                  1|                               0|             0|              1|              0|         0|           C|\n",
      "|YidceGJjXHhiNVx4Y...|    Building Manager|      11.0|   51000.0|             0.0|   801xx|        CO|     3237.0|    16184.0|   1664|        Current|     1200.0|    5|       1200.0| 36 months|  0.1942|   16|     18 June 2018|     false|      2|  Debt consolidation|                 0|                  1|                      0|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           D|\n",
      "|YidceGFkXHg4ZVx4O...|Human Resource Ma...|       9.0|   58000.0|             0.0|   146xx|        NY|     7212.0|   122608.0|  70546|        Current|     8400.0|   33|       8400.0| 36 months|  0.1049|    7| 16 November 2016|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              1|              0|         0|           B|\n",
      "|Yiczflx4YjUqXHhlN...|             Teacher|       0.5|  120000.0|             0.0|   891xx|        NV|    20113.0|   201125.0| 253983|     Fully Paid|    35000.0|   32|      35000.0| 36 months|  0.0724|    1|  17 January 2017|     false|      3|    Home improvement|                 0|                  0|                      1|                 0|                           1|                                  0|                               0|             0|              0|              0|         0|           A|\n",
      "|YiJyJ1x4ODZceDAwX...| Deportation officer|      11.0|  115000.0|             0.0|   919xx|        CA|    66538.0|   598839.0| 243836|     Fully Paid|    30000.0|    4|      30000.0| 60 months|  0.1008|    6|18 September 2018|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           1|                                  0|                               0|             0|              0|              0|         0|           B|\n",
      "|YicrXHgxOFx4Y2Nce...|Internet Sales Co...|       0.5|   37000.0|             0.0|   805xx|        CO|    22918.0|   206261.0|  86629|     Fully Paid|    10000.0|    5|      10000.0| 36 months|  0.0818|    7|  15 October 2015|     false|      1|Credit card refin...|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|           B|\n",
      "+--------------------+--------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+---------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_fintech_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|    original_column| original_value|      encoded_column|\n",
      "+-------------------+---------------+--------------------+\n",
      "|     home_ownership|            OWN|  home_ownership_OWN|\n",
      "|     home_ownership|           RENT| home_ownership_RENT|\n",
      "|     home_ownership|       MORTGAGE|home_ownership_MO...|\n",
      "|     home_ownership|            ANY|  home_ownership_ANY|\n",
      "|verification_status|       Verified|verification_stat...|\n",
      "|verification_status|Source Verified|verification_stat...|\n",
      "|verification_status|   Not Verified|verification_stat...|\n",
      "|               type|      Joint App|      type_Joint App|\n",
      "|               type|     Individual|     type_Individual|\n",
      "|               type|     DIRECT_PAY|     type_DIRECT_PAY|\n",
      "|               type|          JOINT|          type_JOINT|\n",
      "|               type|     INDIVIDUAL|     type_INDIVIDUAL|\n",
      "|              state|             AK|                   0|\n",
      "|              state|             AL|                   1|\n",
      "|              state|             AR|                   2|\n",
      "|              state|             AZ|                   3|\n",
      "|              state|             CA|                   4|\n",
      "|              state|             CO|                   5|\n",
      "|              state|             CT|                   6|\n",
      "|              state|             DC|                   7|\n",
      "|              state|             DE|                   8|\n",
      "|              state|             FL|                   9|\n",
      "|              state|             GA|                  10|\n",
      "|              state|             HI|                  11|\n",
      "|              state|             ID|                  12|\n",
      "|              state|             IL|                  13|\n",
      "|              state|             IN|                  14|\n",
      "|              state|             KS|                  15|\n",
      "|              state|             KY|                  16|\n",
      "|              state|             LA|                  17|\n",
      "|              state|             MA|                  18|\n",
      "|              state|             MD|                  19|\n",
      "|              state|             ME|                  20|\n",
      "|              state|             MI|                  21|\n",
      "|              state|             MN|                  22|\n",
      "|              state|             MO|                  23|\n",
      "|              state|             MS|                  24|\n",
      "|              state|             MT|                  25|\n",
      "|              state|             NC|                  26|\n",
      "|              state|             ND|                  27|\n",
      "|              state|             NE|                  28|\n",
      "|              state|             NH|                  29|\n",
      "|              state|             NJ|                  30|\n",
      "|              state|             NM|                  31|\n",
      "|              state|             NV|                  32|\n",
      "|              state|             NY|                  33|\n",
      "|              state|             OH|                  34|\n",
      "|              state|             OK|                  35|\n",
      "|              state|             OR|                  36|\n",
      "|              state|             PA|                  37|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_LOOKUP_TABLE.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that adds the 3 following features. Try as much as you can to use\n",
    "built in fucntions in PySpark (from the functions library) check lab 8, Avoid writing\n",
    "UDFs from scratch.\n",
    "- Previous loan issue date from the same grade\n",
    "- Previoius Loan amount from the same grade\n",
    "- Previous loan date from the same state and grade combined\n",
    "- Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan issue date from the same grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous_loan_issue_date_form_same_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_issue_date_same_grade' that contains the issue date of the previous loan with the same grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_issue_date_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_issue_date = fn.lag(\"issue_date\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_issue_date_same_grade\", prev_issue_date)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df = add_previous_loan_issue_date_form_same_grade(encoded_fintech_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------------------------+\n",
      "|       issue_date|grade|previous_loan_issue_date_same_grade|\n",
      "+-----------------+-----+-----------------------------------+\n",
      "|12 September 2012|    1|                               NULL|\n",
      "|12 September 2012|    1|                  12 September 2012|\n",
      "|12 September 2012|    1|                  12 September 2012|\n",
      "|  12 October 2012|    1|                  12 September 2012|\n",
      "| 12 November 2012|    1|                    12 October 2012|\n",
      "| 12 December 2012|    1|                   12 November 2012|\n",
      "| 12 December 2012|    1|                   12 December 2012|\n",
      "|  13 January 2013|    1|                   12 December 2012|\n",
      "|  13 January 2013|    1|                    13 January 2013|\n",
      "| 13 February 2013|    1|                    13 January 2013|\n",
      "| 13 February 2013|    1|                   13 February 2013|\n",
      "|    13 March 2013|    1|                   13 February 2013|\n",
      "|    13 March 2013|    1|                      13 March 2013|\n",
      "|    13 March 2013|    1|                      13 March 2013|\n",
      "|    13 April 2013|    1|                      13 March 2013|\n",
      "|    13 April 2013|    1|                      13 April 2013|\n",
      "|    13 April 2013|    1|                      13 April 2013|\n",
      "|      13 May 2013|    1|                      13 April 2013|\n",
      "|      13 May 2013|    1|                        13 May 2013|\n",
      "|     13 June 2013|    1|                        13 May 2013|\n",
      "+-----------------+-----+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df.select(\"issue_date\", \"grade\", \"previous_loan_issue_date_same_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previoius Loan amount from the same grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_amount_from_same_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_amount_same_grade' that contains the loan amount of the previous loan with the same grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"loan_amount\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_amount_same_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_2 = add_prev_loan_amount_from_same_grade(lagged_fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----+-------------------------------+\n",
      "|       issue_date|loan_amount|grade|previous_loan_amount_same_grade|\n",
      "+-----------------+-----------+-----+-------------------------------+\n",
      "|12 September 2012|    12000.0|    1|                           NULL|\n",
      "|12 September 2012|    11200.0|    1|                        12000.0|\n",
      "|12 September 2012|     6500.0|    1|                        11200.0|\n",
      "|  12 October 2012|     8000.0|    1|                         6500.0|\n",
      "| 12 November 2012|    10000.0|    1|                         8000.0|\n",
      "| 12 December 2012|    24000.0|    1|                        10000.0|\n",
      "| 12 December 2012|    24000.0|    1|                        24000.0|\n",
      "|  13 January 2013|    16850.0|    1|                        24000.0|\n",
      "|  13 January 2013|    12000.0|    1|                        16850.0|\n",
      "| 13 February 2013|    10000.0|    1|                        12000.0|\n",
      "| 13 February 2013|    28000.0|    1|                        10000.0|\n",
      "|    13 March 2013|    21000.0|    1|                        28000.0|\n",
      "|    13 March 2013|    16000.0|    1|                        21000.0|\n",
      "|    13 March 2013|    12500.0|    1|                        16000.0|\n",
      "|    13 April 2013|    17400.0|    1|                        12500.0|\n",
      "|    13 April 2013|    24000.0|    1|                        17400.0|\n",
      "|    13 April 2013|    14000.0|    1|                        24000.0|\n",
      "|      13 May 2013|    14400.0|    1|                        14000.0|\n",
      "|      13 May 2013|    24000.0|    1|                        14400.0|\n",
      "|     13 June 2013|     6000.0|    1|                        24000.0|\n",
      "+-----------------+-----------+-----+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_2.select(\"issue_date\",\"loan_amount\", \"grade\", \"previous_loan_amount_same_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan date from the same state and grade combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_date_from_same_state_and_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_date_same_state_and_grade' that contains the issue date of the previous loan with the same state and grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"state\",\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"issue_date\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_date_same_state_and_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_3 = add_prev_loan_date_from_same_state_and_grade(lagged_fintech_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+---------------------------------------+\n",
      "|       issue_date|state|grade|previous_loan_date_same_state_and_grade|\n",
      "+-----------------+-----+-----+---------------------------------------+\n",
      "|  14 October 2014|    0|    1|                                   NULL|\n",
      "|  19 January 2019|    0|    1|                        14 October 2014|\n",
      "| 19 November 2019|    0|    1|                        19 January 2019|\n",
      "|  14 October 2014|    0|    2|                                   NULL|\n",
      "|     17 June 2017|    0|    2|                        14 October 2014|\n",
      "| 17 November 2017|    0|    2|                           17 June 2017|\n",
      "|    18 March 2018|    0|    2|                       17 November 2017|\n",
      "|15 September 2015|    0|    3|                                   NULL|\n",
      "|  18 October 2018|    0|    3|                      15 September 2015|\n",
      "|    19 April 2019|    0|    3|                        18 October 2018|\n",
      "|     16 June 2016|    0|    4|                                   NULL|\n",
      "|      17 May 2017|    0|    4|                           16 June 2016|\n",
      "| 14 November 2014|    0|    5|                                   NULL|\n",
      "|  15 January 2015|    0|    5|                       14 November 2014|\n",
      "|  18 October 2018|    0|    5|                        15 January 2015|\n",
      "|  18 October 2018|    0|    5|                        18 October 2018|\n",
      "|     17 July 2017|    0|    6|                                   NULL|\n",
      "|     16 July 2016|    0|    7|                                   NULL|\n",
      "|   16 August 2016|    0|    7|                           16 July 2016|\n",
      "| 19 February 2019|    0|    7|                         16 August 2016|\n",
      "+-----------------+-----+-----+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_3.select(\"issue_date\",\"state\",\"grade\", \"previous_loan_date_same_state_and_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_loan_amount_from_same_state_and_grade(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column 'previous_loan_amount_same_state_and_grade' that contains the loan amount of the previous loan with the same state and grade.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with the new column 'previous_loan_amount_same_state_and_grade'.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"issue_date_preprocessed\", fn.to_date(fn.col(\"issue_date\"), \"dd MMMM yyyy\"))\n",
    "    window_spec = Window.partitionBy(\"state\",\"grade\").orderBy(\"issue_date_preprocessed\")\n",
    "    prev_loan_amount = fn.lag(\"loan_amount\", 1).over(window_spec)\n",
    "    df = df.withColumn(\"previous_loan_amount_same_state_and_grade\", prev_loan_amount)\n",
    "    df = df.drop(\"issue_date_preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_fintech_df_4 = add_prev_loan_amount_from_same_state_and_grade(lagged_fintech_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "|       issue_date|state|loan_amount|grade|previous_loan_amount_same_state_and_grade|\n",
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "|  14 October 2014|    0|    10000.0|    1|                                     NULL|\n",
      "|  19 January 2019|    0|    10000.0|    1|                                  10000.0|\n",
      "| 19 November 2019|    0|     6000.0|    1|                                  10000.0|\n",
      "|  14 October 2014|    0|    24000.0|    2|                                     NULL|\n",
      "|     17 June 2017|    0|    30000.0|    2|                                  24000.0|\n",
      "| 17 November 2017|    0|    21375.0|    2|                                  30000.0|\n",
      "|    18 March 2018|    0|    35000.0|    2|                                  21375.0|\n",
      "|15 September 2015|    0|    15000.0|    3|                                     NULL|\n",
      "|  18 October 2018|    0|    25000.0|    3|                                  15000.0|\n",
      "|    19 April 2019|    0|    40000.0|    3|                                  25000.0|\n",
      "|     16 June 2016|    0|    15000.0|    4|                                     NULL|\n",
      "|      17 May 2017|    0|    16000.0|    4|                                  15000.0|\n",
      "| 14 November 2014|    0|    12000.0|    5|                                     NULL|\n",
      "|  15 January 2015|    0|    25000.0|    5|                                  12000.0|\n",
      "|  18 October 2018|    0|    32000.0|    5|                                  25000.0|\n",
      "|  18 October 2018|    0|    33600.0|    5|                                  32000.0|\n",
      "|     17 July 2017|    0|    11000.0|    6|                                     NULL|\n",
      "|     16 July 2016|    0|    12000.0|    7|                                     NULL|\n",
      "|   16 August 2016|    0|    12000.0|    7|                                  12000.0|\n",
      "| 19 February 2019|    0|    10000.0|    7|                                  12000.0|\n",
      "+-----------------+-----+-----------+-----+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagged_fintech_df_4.select(\"issue_date\",\"state\",\"loan_amount\", \"grade\", \"previous_loan_amount_same_state_and_grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Lookup Table & Saving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load (save) the cleaned PySpark df and the lookup table to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_parquet(df: pyspark.sql.dataframe.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a PySpark DataFrame to disk in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    path (str): Output path for the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df.write.mode(\"overwrite\").parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ./work/fintech_spark_52_1008_clean.parquet\n",
      "DataFrame saved to ./work/lookup_spark_52_1008.parquet\n"
     ]
    }
   ],
   "source": [
    "save_df_as_parquet(lagged_fintech_df_4, \"./work/fintech_spark_52_1008_clean.parquet\")\n",
    "save_df_as_parquet(GLOBAL_LOOKUP_TABLE, \"./work/lookup_spark_52_1008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_parquet(path: str) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a PySpark DataFrame from disk in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "    path (str): Path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Loaded PySpark DataFrame.\n",
    "    \"\"\"\n",
    "    return spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_fintech_df = load_df_from_parquet(\"./work/fintech_spark_52_1008_clean.parquet\")\n",
    "loaded_lookup_table = load_df_from_parquet(\"./work/lookup_spark_52_1008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "|         customer_id|          emp_title|emp_length|annual_inc|annual_inc_joint|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|      issue_date|pymnt_plan|purpose|         description|home_ownership_OWN|home_ownership_RENT|home_ownership_MORTGAGE|home_ownership_ANY|verification_status_Verified|verification_status_Source Verified|verification_status_Not Verified|type_Joint App|type_INDIVIDUAL|type_DIRECT_PAY|type_JOINT|letter_grade|previous_loan_issue_date_same_grade|previous_loan_amount_same_grade|previous_loan_date_same_state_and_grade|previous_loan_amount_same_state_and_grade|\n",
      "+--------------------+-------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "|YiJOXHhlYlx4ZjFce...|             Safety|       6.0|  100000.0|             0.0|   996xx|        AK|    26004.0|   260041.0| 147101|    Current|    15000.0|    0|      15000.0| 36 months|  0.0739|    4|    16 June 2016|     false|      3|    Home improvement|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              1|              0|         0|           A|                       16 June 2016|                        29400.0|                                   NULL|                                     NULL|\n",
      "|YidceGNhWFhceGU1X...|         IT Analyst|       2.0|   83000.0|             0.0|   995xx|        AK|    73708.0|   294830.0| 163274|    Current|    16000.0|    0|      16000.0| 36 months|  0.0797|    4|     17 May 2017|     false|      5|      Major purchase|                 0|                  0|                      1|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           A|                        17 May 2017|                         9000.0|                           16 June 2016|                                  15000.0|\n",
      "|YidceDA2XHg4Zlx4Y...|                 RN|      11.0|  115000.0|             0.0|   995xx|        AK|    12795.0|   243107.0| 193189| Fully Paid|    20000.0|    0|      20000.0| 36 months|  0.1229|   13| 15 October 2015|     false|      2|  Debt consolidation|                 0|                  0|                      1|                 0|                           1|                                  0|                               0|             0|              0|              0|         0|           C|                    15 October 2015|                         5000.0|                                   NULL|                                     NULL|\n",
      "|YidceGU5NFx4MTFce...|            Teacher|       4.0|   24262.0|             0.0|   996xx|        AK|     1823.0|     7291.0| 111224|    Current|    11000.0|    0|      11000.0| 60 months|  0.1612|   13|19 December 2019|     false|      2|  Debt consolidation|                 1|                  0|                      0|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|           C|                   19 December 2019|                        11000.0|                        15 October 2015|                                  20000.0|\n",
      "|YidceDlhXHg4MVx4Y...|Enforcement Officer|      11.0|  105000.0|             0.0|   998xx|        AK|    39327.0|   235959.0| 159819|    Current|    15350.0|    0|      15350.0| 60 months|  0.1624|   14|13 December 2013|     false|      2| debt consolidati...|                 0|                  0|                      1|                 0|                           0|                                  1|                               0|             0|              1|              0|         0|           C|                   13 December 2013|                        35000.0|                                   NULL|                                     NULL|\n",
      "+--------------------+-------------------+----------+----------+----------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+-------+--------------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+------------+-----------------------------------+-------------------------------+---------------------------------------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_fintech_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+\n",
      "|    original_column| original_value|      encoded_column|\n",
      "+-------------------+---------------+--------------------+\n",
      "|verification_status|Source Verified|verification_stat...|\n",
      "|verification_status|   Not Verified|verification_stat...|\n",
      "|verification_status|       Verified|verification_stat...|\n",
      "|     home_ownership|       MORTGAGE|home_ownership_MO...|\n",
      "|     home_ownership|           RENT| home_ownership_RENT|\n",
      "+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_lookup_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Loading to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the cleaned parquet file and lookup table into a Postgres database.\n",
    "- Take Screenshots showing the newly added features in the feature engineering section\n",
    "- Take a screenshot from the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_db(df: pyspark.sql.dataframe.DataFrame, table_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a PySpark DataFrame to a database table.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input PySpark DataFrame.\n",
    "    table_name (str): Name of the database table.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    postgres_url = \"jdbc:postgresql://pgdatabase:5432/testdb\"\n",
    "    postgres_properties = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"root\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    df.write.jdbc(\n",
    "        url=postgres_url,\n",
    "        table=table_name,\n",
    "        mode=\"overwrite\",  # Options: 'overwrite', 'append', 'ignore', 'error'\n",
    "        properties=postgres_properties\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_db(lagged_fintech_df_4, \"fintech_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
